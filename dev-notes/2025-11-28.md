# RAG Learning Notes - 2025-11-28

## 1. What is Chunk Overlap? Why is chunk_overlap=0 not ideal?

### Definition of Chunk Overlap
Chunk overlap is the number of characters (or tokens) shared between adjacent chunks when splitting text.

```
Example: chunk_size=100, chunk_overlap=20

Chunk 1: [char 0-99]
Chunk 2: [char 80-179]  <- chars 80-99 overlap
Chunk 3: [char 160-259] <- chars 160-179 overlap
```

### Why chunk_overlap=0 is problematic

1. **Context breaks**: Important information may be split at chunk boundaries
   ```
   Example: "Tanaka-san lives in Tokyo. He is | an engineer."
            ^ If chunk breaks here, we lose who "He" refers to
   ```

2. **Lower search accuracy**: Related information in separate chunks may not be retrieved together

3. **Recommended value**: 10-20% of chunk_size (e.g., chunk_size=1000 -> overlap=100-200)

---

## 2. Text Splitters - Types and When to Use

### Major Splitter Comparison

| Splitter | Features | Use Case |
|----------|----------|----------|
| **CharacterTextSplitter** | Simple character count split | Simple text, testing |
| **RecursiveCharacterTextSplitter** | Hierarchically finds split points (paragraph -> sentence -> word) | **Most versatile, recommended default** |
| **MarkdownTextSplitter** | Splits by Markdown structure (# headings etc.) | README, documentation |
| **HTMLTextSplitter** | Splits by HTML tags (`<p>`, `<div>` etc.) | Web pages |
| **TokenTextSplitter** | Splits by token count | When strictly respecting LLM context limits |
| **SemanticSplitter** | Determines split points by semantic similarity | High-quality RAG needed (high compute cost) |

### Why RecursiveCharacterTextSplitter is recommended

```python
# Default split priority order
separators = ["\n\n", "\n", " ", ""]
# 1. First try to split by paragraph (\n\n)
# 2. If paragraph too large, split by line (\n)
# 3. If still too large, split by word (space)
# 4. Last resort: character-level split
```

This minimizes splitting in the middle of sentences.

### Language-specific support

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter, Language

# For Python code
python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=1000,
    chunk_overlap=200
)

# Also supports JavaScript, HTML, Markdown, etc.
```

---

## 3. What is Chroma? Vector Database Comparison

### Chroma Overview
Chroma is an open-source vector database designed for LLM applications.

### How persist_directory works

```python
# Memory only (no persistence) - data lost when script ends
db = Chroma.from_documents(texts, embeddings)

# With persistence - saved to disk
db = Chroma.from_documents(
    texts,
    embeddings,
    persist_directory="./chroma_db"  # Data saved to this folder
)

# Reload later
db = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)
```

### Vector Database Comparison Table

| DB | Features | Best For | Cost |
|----|----------|----------|------|
| **Chroma** | Simple, Python-focused, prototype-friendly | Learning, small projects, rapid prototyping | **Free (OSS)** |
| **FAISS** | Made by Meta, fast, GPU support, library | Research, max performance | **Free (OSS)** |
| **Pinecone** | Fully managed, high availability, SLA | Production, customer-facing AI | Paid (usage-based) |
| **Weaviate** | GraphQL support, knowledge graph integration | Complex data relationships, semantic search | Free (OSS) / Paid (Cloud) |
| **Milvus** | Large-scale support, 11 index types | Enterprise, large-scale cloud | Free (OSS) / Paid (Zilliz Cloud) |

### Is Chroma the best?

**Best for learning & prototypes.** Reasons:
- Easy setup (`pip install chromadb` only)
- Good LangChain integration
- Runs completely locally

**Consider alternatives for production:**
- Single-node only (scalability limitations)
- For large-scale data, consider Milvus or Pinecone

---

## 4. LLM Comparison (API Pricing & Cost Efficiency)

### OpenAI Pricing (as of 2025)

| Model | Input (1M tokens) | Output (1M tokens) | Features |
|-------|-------------------|-------------------|----------|
| **GPT-4o** | $2.50 | $10.00 | Balanced, recommended for user-facing apps |
| **GPT-4o mini** | $0.15 | $0.60 | Lightweight tasks, excellent cost efficiency |
| **GPT-4 Turbo** | $10.00 | $30.00 | High performance but expensive |
| **GPT-3.5 Turbo** | $0.50 | $1.50 | Legacy, mini recommended now |

### Anthropic Claude Pricing (as of 2025)

| Model | Input (1M tokens) | Output (1M tokens) | Features |
|-------|-------------------|-------------------|----------|
| **Claude 3 Haiku** | $0.25 | $1.25 | **Cheapest, fast, lightweight tasks** |
| **Claude 3.5 Haiku** | $0.80 | $4.00 | Improved Haiku |
| **Claude 3.5 Sonnet** | $3.00 | $15.00 | Balanced, good at coding |
| **Claude 3 Opus** | $15.00 | $75.00 | Highest performance, complex reasoning |

### Google Gemini Pricing (as of 2025)

| Model | Input (1M tokens) | Output (1M tokens) | Features |
|-------|-------------------|-------------------|----------|
| **Gemini 2.5 Flash-Lite** | $0.075 | $0.30 | **Cheapest**, high throughput |
| **Gemini 2.5 Flash** | $0.15 | $0.60 (no reasoning) | Fast, balanced cost |
| **Gemini 2.0 Flash** | $0.10 | $0.40 | Previous gen Flash |
| **Gemini 2.5 Pro** | $1.25 | $10.00 | Most capable, complex tasks |
| **Gemini 1.5 Pro** | $1.25 | $5.00 | Previous gen Pro |

**Gemini Unique Features:**
- **Free tier available** - rate-limited but no cost (data used to improve Google products)
- **Flat pricing** for Flash models regardless of context length
- **Context caching** - cache reads cost only 10% of base input price
- **Multimodal native** - handles text, image, video, audio in one model
- **1M+ token context** - Gemini 1.5 Pro supports up to 2M tokens

**Note:** Gemini 2.5 Flash with reasoning enabled costs $3.50/1M output tokens (higher than non-reasoning)

### Cost Efficiency Ranking (Updated with Gemini)

1. **Cheapest**: Gemini 2.5 Flash-Lite ($0.075/$0.30) - *has free tier!*
2. **Very cheap**: GPT-4o mini ($0.15/$0.60) or Gemini 2.5 Flash ($0.15/$0.60)
3. **Cheap**: Claude 3 Haiku ($0.25/$1.25)
4. **Balanced**: GPT-4o ($2.50/$10), Claude 3.5 Sonnet ($3/$15), Gemini 2.5 Pro ($1.25/$10)
5. **Premium**: Claude Opus ($15/$75)

### Quick Comparison: Which to Choose?

| Need | Best Choice | Why |
|------|-------------|-----|
| **Lowest cost** | Gemini Flash-Lite | $0.075 input, has free tier |
| **Free experimentation** | Gemini (any) | Free tier with rate limits |
| **Japanese text** | Claude | Best Japanese quality |
| **Long documents (1M+ tokens)** | Gemini 1.5/2.5 Pro | Up to 2M context window |
| **Coding tasks** | Claude Sonnet or GPT-4o | Strong code understanding |
| **Multimodal (images/video)** | Gemini | Native multimodal support |
| **Privacy-sensitive** | Claude or OpenAI | Gemini free tier uses your data |

---

## 5. Embedding Models Comparison (OpenAI vs Sentence-Transformers)

### Basic Concept

**What is Embedding?**
Converting text into numerical vectors. Similar texts become similar vectors.

```
"A dog is running"     -> [0.23, -0.45, 0.12, ...] (e.g., 1536 dimensions)
"A canine is jogging"  -> [0.24, -0.44, 0.11, ...] <- similar vector
"Stock prices rose"    -> [-0.67, 0.89, -0.23, ...] <- distant vector
```

### OpenAI Embeddings

| Model | Dimensions | Price (1M tokens) | Features |
|-------|------------|-------------------|----------|
| text-embedding-3-large | 3072 | $0.13 | High accuracy |
| text-embedding-3-small | 1536 | $0.02 | Cost efficient |
| text-embedding-ada-002 | 1536 | $0.10 | Legacy |

**Pros:**
- Excellent multilingual support (including Japanese)
- Good at understanding long text meaning
- Easy setup

**Cons:**
- API dependent (no offline use)
- Usage-based costs add up
- Data passes through OpenAI servers

### Sentence-Transformers (Open Source)

```python
# Installation
pip install sentence-transformers

# Usage example
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(["text 1", "text 2"])
```

**Popular models:**
| Model | Dimensions | Features |
|-------|------------|----------|
| all-MiniLM-L6-v2 | 384 | Lightweight, fast, general purpose |
| all-mpnet-base-v2 | 768 | Higher accuracy |
| multilingual-e5-large | 1024 | Excellent multilingual |
| paraphrase-multilingual-MiniLM-L12-v2 | 384 | Multilingual, lightweight |

**Pros:**
- **Completely free** (local execution)
- Privacy protection (data stays local)
- Offline capable
- Very fast with GPU

**Cons:**
- Initial model download required
- Some models are English-focused
- Self-managed infrastructure needed

### Recommendations

| Scenario | Recommendation |
|----------|----------------|
| Learning & experimentation | Sentence-Transformers (free) |
| Japanese-focused | OpenAI text-embedding-3-small or multilingual-e5 |
| Privacy-focused | Sentence-Transformers |
| Production, large-scale | OpenAI (stability) or self-hosting |

---

## 6. Document Type Considerations

### Plain Text (.txt)

**Features:**
- Simplest, no structural information
- Can chunk directly

**Recommended Splitter:** RecursiveCharacterTextSplitter

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
```

---

### Markdown (.md)

**Features:**
- Clear structure with headings (#)
- Has code blocks, lists, etc.

**Recommended Splitter:** MarkdownTextSplitter or MarkdownHeaderTextSplitter

```python
from langchain.text_splitter import MarkdownHeaderTextSplitter

headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]
splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
```

**Notes:**
- Leverage heading hierarchy to preserve metadata
- Consider separate processing for code blocks

---

### PDF (.pdf)

**Features:**
- Visual format (complex text extraction)
- Tables, images, multiple columns, headers/footers

**Challenges:**
- Text extraction quality depends on tools
- Scanned PDFs require OCR

**Recommended Approach:**
1. **Convert PDF to Markdown** before chunking
2. Tools: `pymupdf4llm`, `unstructured`, `docling`

```python
# Example using pymupdf4llm
import pymupdf4llm
md_text = pymupdf4llm.to_markdown("document.pdf")

# Then split with MarkdownTextSplitter
```

**Notes:**
- Table data needs special handling
- Text in images requires OCR

---

### HTML (.html)

**Features:**
- Structured with tags
- Often has unwanted elements (nav, ads, etc.)

**Recommended Splitter:** HTMLTextSplitter + preprocessing

```python
from langchain.text_splitter import HTMLHeaderTextSplitter

splitter = HTMLHeaderTextSplitter(
    headers_to_split_on=[
        ("h1", "Header 1"),
        ("h2", "Header 2"),
    ]
)
```

**Notes:**
- Remove unwanted elements with BeautifulSoup before processing
- Extract main content only

---

### Code (.py, .js, etc.)

**Features:**
- Meaningful at function/class level
- Simple character splitting breaks code

**Recommended Splitter:** RecursiveCharacterTextSplitter.from_language

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter, Language

python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=1000,
    chunk_overlap=200
)
```

**Notes:**
- Preserve function/class units
- Include import statements and context

---

### Mixed Document Types

**Challenges:**
- Each type needs different optimal processing
- Unified search is difficult

**Recommended Approach:**

```python
import os
from pathlib import Path

def process_document(file_path: str):
    ext = Path(file_path).suffix.lower()

    if ext == '.txt':
        return process_text(file_path)
    elif ext == '.md':
        return process_markdown(file_path)
    elif ext == '.pdf':
        return process_pdf(file_path)
    elif ext == '.html':
        return process_html(file_path)
    elif ext in ['.py', '.js', '.ts']:
        return process_code(file_path)
    else:
        raise ValueError(f"Unsupported file type: {ext}")
```

**Best Practices:**
1. **File type detection** -> Select appropriate loader
2. **Add metadata** -> Source filename, type, timestamp, etc.
3. **Unified format conversion** -> Convert to Markdown if possible
4. **Type-specific chunk settings** -> Larger for PDFs, smaller for code, etc.

---

## Summary: Recommended Stack for Learners

```python
# Recommended setup (for learning & prototypes)

# 1. Embedding: Free Sentence-Transformers
from sentence_transformers import SentenceTransformer
embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# 2. Vector DB: Simple Chroma (with persistence)
import chromadb
client = chromadb.PersistentClient(path="./chroma_db")

# 3. Text Splitter: Versatile Recursive
from langchain.text_splitter import RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

# 4. LLM: Cost-efficient Claude 3 Haiku or GPT-4o mini
```

---

## References

### LLM Pricing
- [OpenAI Pricing](https://openai.com/api/pricing/)
- [Anthropic Claude Pricing](https://www.anthropic.com/pricing)

### Vector Databases
- [Vector Database Comparison 2025](https://medium.com/tech-ai-made-easy/vector-database-comparison-pinecone-vs-weaviate-vs-qdrant-vs-faiss-vs-milvus-vs-chroma-2025-15bf152f891d)
- [Chroma Documentation](https://docs.trychroma.com/)

### Embeddings
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [Sentence-Transformers](https://www.sbert.net/)
- [Embedding Models Comparison](https://markaicode.com/embedding-models-comparison-openai-sentence-transformers/)

### Chunking Best Practices
- [Chunking for RAG - Unstructured](https://unstructured.io/blog/chunking-for-rag-best-practices)
- [Breaking up is hard to do - Stack Overflow](https://stackoverflow.blog/2024/12/27/breaking-up-is-hard-to-do-chunking-in-rag-applications/)
- [Weaviate Chunking Strategies](https://weaviate.io/blog/chunking-strategies-for-rag)

---

## Today's Achievements

### Project Setup Complete

Created a working RAG project with the following stack:

| Component | Choice | Reason |
|-----------|--------|--------|
| **Embedding** | sentence-transformers (paraphrase-multilingual-MiniLM-L12-v2) | Free, local, multilingual |
| **Vector DB** | Chroma (with persistence) | Simple, good for learning |
| **LLM** | Google Gemini 2.0 Flash | Free tier available |
| **Framework** | LangChain | Most tutorials use it |
| **Splitter** | RecursiveCharacterTextSplitter | Best general-purpose option |

### Files Created

```
rag-practice/
├── pyproject.toml      # Dependencies (langchain, chromadb, sentence-transformers, etc.)
├── main.py             # Complete RAG implementation with interactive loop
├── .env.example        # Template for GOOGLE_API_KEY
├── .gitignore          # Excludes chroma_db/, .env
└── documents/
    └── sample.txt      # Sample document about RAG concepts
```

### Key Features of main.py

1. **Document Loading**: Loads all `.txt` files from `documents/` folder
2. **Chunking**: Uses RecursiveCharacterTextSplitter (chunk_size=1000, overlap=200)
3. **Local Embeddings**: sentence-transformers runs locally (no API cost)
4. **Persistent Storage**: Chroma saves to `chroma_db/` folder
5. **Interactive Loop**: Ask questions, type 'quit' to exit, 'rebuild' to reload docs
6. **Error Handling**: Clear messages for missing API key

### How to Run

```bash
# 1. Install dependencies
uv sync

# 2. Set up API key
cp .env.example .env
# Edit .env and add your Gemini API key from https://aistudio.google.com/apikey

# 3. Run
uv run python main.py

# 4. Ask questions!
# Example: "What is RAG?"
# Example: "What are the benefits of RAG?"
```

### npm-like Script Shortcut with uv

**Goal:** Run `uv run rag` instead of `uv run python main.py`

**Problem:** Python's `[project.scripts]` is NOT like npm scripts. It creates executable entry points that require the package to be installed as a proper Python package.

**Solution:** Add a build system and tell hatchling where the code is.

```toml
# pyproject.toml

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["."]  # Include current directory as package

[project.scripts]
rag = "main:main"  # Maps "rag" command to main() function in main.py
```

**Key differences from npm:**

| npm | uv/Python |
|-----|-----------|
| `"scripts": {"start": "node app.js"}` | `[project.scripts]` + build system |
| Runs shell commands | Creates actual executable entry points |
| No build step | Requires package installation |
| `npm run start` | `uv run rag` |

**Why it's more complex:**
- Python entry points are "real" executables installed in the environment
- npm scripts are just shell command aliases
- uv doesn't have npm-like script aliases yet ([requested feature](https://github.com/astral-sh/uv/issues/8122))

**After setup:**
```bash
uv sync        # Install package (creates entry point)
uv run rag     # Works!
```

### uv vs PDM Comparison

| Feature | uv | PDM |
|---------|-----|-----|
| Speed | Extremely fast (Rust) | Fast (Python) |
| npm-like scripts | No (entry points only) | Yes (pre_*, post_* hooks) |
| Maturity | Newer (2024) | More mature (2021) |
| Lockfile | uv.lock | pdm.lock |
| PEP compliance | Strong | Strong |
| Virtual env | Auto-managed | Auto-managed |
| Global tool install | uv tool install | pdm install -g |
| Monorepo support | Workspaces | Workspaces |
| Backed by | Astral (ruff makers) | Community |

**When to choose uv:**
- Speed is priority (10-100x faster than pip)
- Simple projects without complex scripts
- You like the Astral ecosystem (ruff, etc.)
- You want the "future" of Python tooling

**When to choose PDM:**
- You need npm-like pre/post hooks
- You want mature, stable tooling
- Complex script workflows
- Coming from npm and want familiar patterns

**Python command hooks:**

Python/uv doesn't have built-in pre/post hooks like npm. Alternatives:

| Tool | Hook Support | Notes |
|------|--------------|-------|
| Hatch | Yes | Chained commands in scripts |
| PDM | Yes | pre_* and post_* hooks |
| Poetry | No | No native hooks |
| uv | No | No hooks yet |
| pre-commit | Yes | Git hooks only |
| Makefile | Yes | Manual but universal |

**Recommendation:** For learning, stick with uv. The speed advantage is significant and it's rapidly improving. If you need complex CI/CD workflows with hooks, consider PDM or use a Makefile.

### Deep Dive: Why Embeddings Need AI

**Question:** Why can't we just convert text to vectors with a simple algorithm?

**Answer:** A simple conversion (like ASCII codes) captures no semantic meaning:

```
# Naive approach - no meaning
"cat" -> [99, 97, 116]
"dog" -> [100, 111, 103]
# "cat" and "feline" would look completely unrelated!
```

**What embedding models do:**

They learn from billions of sentences that words appearing in similar contexts should have similar vectors:

```
"cat"      -> [0.82, 0.15, -0.34, ...]
"feline"   -> [0.79, 0.18, -0.31, ...]  # Very close!
"dog"      -> [0.71, 0.22, -0.28, ...]  # Close (both pets)
"airplane" -> [-0.45, 0.67, 0.12, ...]  # Far away
```

The ~500MB model download IS the pre-trained knowledge - millions of learned weights that encode language understanding.

---

### Deep Dive: RAG vs LLM Training

**Question:** Are models like GPT/Claude trained using RAG? Why do we need RAG if LLMs already "know" things?

**Answer:** No, LLMs are NOT trained with RAG. They are fundamentally different:

| Aspect | LLM Training | RAG |
|--------|--------------|-----|
| When | Pre-training (months, once) | Runtime (every query) |
| How | Learn patterns, encode into weights | Retrieve docs, feed to LLM |
| Storage | Knowledge "baked into" parameters | External vector database |
| Update | Requires retraining (expensive) | Just add new documents |

**Why RAG exists:**

1. **Knowledge cutoff**: LLMs don't know what happened after training
2. **No access to YOUR data**: LLM never saw your company docs
3. **Hallucination**: RAG grounds answers in actual documents
4. **Cost**: Retraining costs millions; RAG just adds documents

**Think of it like:**
- LLM alone = Smart person answering from memory (might make things up)
- LLM + RAG = Same person, but you hand them relevant pages first

---

### Deep Dive: RAG vs Fine-tuning

**Question:** When should a company use RAG vs fine-tuning?

| Factor | RAG | Fine-tuning |
|--------|-----|-------------|
| Cost to update | Add documents (free) | Retrain ($1000s+) |
| Data freshness | Real-time updates | Frozen at training |
| Hallucination | Lower (cites docs) | Higher |
| Transparency | Can show sources | Black box |
| Privacy | Data stays in your DB | Data used in training |

**When to use what:**

| Need | Solution |
|------|----------|
| Access specific documents | RAG |
| Different writing style/tone | Fine-tune |
| New factual knowledge | RAG |
| Custom output format | Fine-tune |
| Real-time updates | RAG |

**Industry pattern:** Most companies do RAG first. Fine-tuning is for style/format, not facts.

---

### Deep Dive: How Fine-tuning Works

**Concept:** Continue training a pre-trained LLM on your specific data.

**Methods:**

| Method | What it does | Cost |
|--------|--------------|------|
| Full fine-tuning | Adjust all weights | Very high ($10k+) |
| LoRA/QLoRA | Adjust small adapter layers | Low ($10-100) |
| RLHF | Train with human feedback | Very high |
| Prompt tuning | Learn soft prompts only | Low |

**LoRA (Low-Rank Adaptation)** - Industry standard:

```
Original LLM weights (frozen)
  + Small adapter weights (trained, ~1-5% of model size)
  = Fine-tuned behavior
```

**Example with OpenAI:**

```python
# 1. Prepare training data (JSONL)
{"messages": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}

# 2. Fine-tune
openai.FineTuningJob.create(training_file="file-xxx", model="gpt-4o-mini")

# 3. Use your custom model
openai.ChatCompletion.create(model="ft:gpt-4o-mini:your-org:custom-name")
```

---

### OpenAI Web Search = RAG

**Observation:** OpenAI's web search feature with citations looks like RAG.

**Answer:** Yes, it's the same pattern:

```
User question
  -> Search the web (retrieval)
  -> Get relevant pages
  -> Feed to LLM (generation)
  -> Answer with citations
```

Same concept, just using the internet as the "vector database."

---

### Next Steps

- [ ] Add support for PDF documents
- [ ] Add support for Markdown documents
- [ ] Experiment with different embedding models
- [ ] Try different chunk sizes and measure quality
- [ ] Add source citation to responses
