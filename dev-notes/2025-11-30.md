# RAG Learning Notes - 2025-11-30

## Web Search IS RAG

When an LLM uses a search tool (like Google Search), it is performing **Agentic RAG** (or "On-Demand RAG"):
1. Generate a query
2. Retrieve external data
3. Use that data to generate an answer

This is the same pattern as RAG with a vector database.

---

## Why Build RAG Pipelines When Tool Use Exists?

The answer: **Scale**, **Cost**, and **The "Needle in a Haystack" Problem**.

### The "Read" vs. "Search" Problem

You cannot "read" what you haven't "found."

| Scale | Approach |
|-------|----------|
| **Small (5 PDFs)** | Just give LLM a `read_file()` tool. No RAG needed. |
| **Large (1M docs)** | Need a search engine to find the top 5 relevant pages first. That search engine IS RAG. |

Even if you wrap search in a "tool," that tool essentially becomes a RAG engine.

---

## Standard RAG vs Agentic RAG

### Standard RAG (Pipeline Approach)

```
User asks question â†’ Search Database â†’ Paste results into Prompt â†’ Send to LLM
```

**Pros:**
- Speed: Single pass (200ms search + 2s generation)
- Cost: One LLM call
- Control: Engineer controls what chunks are retrieved

### Agentic RAG (Tool Use Approach)

```
User asks question â†’ LLM thinks â†’ LLM calls Search Tool â†’ Tool returns data â†’ LLM reads â†’ LLM answers
```

**Pros:**
- Flexibility: Can search multiple times if first result is bad
- Reasoning: Multi-hop reasoning (search X, realize need Y, search Y)

**Cons:**
- Slower (multiple round-trips)
- More expensive (more tokens)

### Comparison

| Feature | Standard RAG | Agentic RAG |
|---------|--------------|-------------|
| Mechanism | Hard-coded pipeline | Dynamic loop (LLM decides) |
| Best For | Static knowledge bases | Complex research, multi-step problems |
| Latency | Fast | Slow |
| Search Engine | Required | Required |

---

## The LLM-VectorDB Communication Flow

**The LLM does NOT talk directly to the Vector Database.**

- Vector DB speaks "Numbers" (`[0.1, -0.9, 0.4...]`)
- LLM speaks "Text"

### The 3-Step Flow

1. **Translation (Embedding Model)**
   - User: "How do I reset my router?"
   - Embedding Model: `[0.02, -0.45, 0.88, ...]`

2. **Retrieval (Vector Database)**
   - Query vector sent to Vector DB
   - DB performs Cosine Similarity search to find similar vectors
   - Returns the **original text** stored alongside matching vectors (not converted back - text is stored as metadata during indexing)

3. **Synthesis (LLM)**
   - Retrieved text pasted into prompt
   - LLM generates answer

### LangChain's Role

LangChain is the "Manager" that orchestrates this pipeline. It handles:
- Embedding â†’ Vector Search â†’ Prompt Formatting â†’ LLM

```python
# Raw Python: ~50 lines of glue code
# LangChain: ~5 lines
qa_chain = RetrievalQA.from_chain_type(
    llm=chat_model,
    retriever=vector_store.as_retriever()
)
response = qa_chain.invoke("How do I reset my router?")
```

---

## LangChain Does NOT Replace Models

**Misconception:** LangChain handles embeddings automatically.

**Reality:** LangChain is like a TV Remote - it has buttons, but needs a TV (the model) to work.

| Component | Role | Has a "Brain"? |
|-----------|------|----------------|
| Embedding Model | The Worker/Translator | **Yes** (knows "Dog" â‰ˆ "Puppy") |
| Vector DB | The Storage/Bookshelf | No (just holds numbers) |
| LangChain | The Manager/Clipboard | No (just orchestrates) |

```python
# You're still loading a model here!
from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()  # Calls OpenAI's servers
```

---

## Chains vs Agents in LangChain

### Chains (Fixed Logic)

- **Logic:** Fixed (A â†’ B â†’ C)
- **Analogy:** Train on tracks
- **Use:** Reliability ("Always Read PDF â†’ Summarize")
- **Who writes the loop:** You (hard-coded Python)

### Agents (Dynamic Logic)

- **Logic:** Dynamic (LLM decides steps at runtime)
- **Analogy:** Taxi driver choosing route based on traffic
- **Use:** Unknown steps ("Book me a flight to Paris")
- **Who writes the loop:** The LLM

### Comparison

| Feature | Chains (Fixed) | Agents (Dynamic) |
|---------|----------------|------------------|
| Predictability | High (100% same path) | Low (varies by data) |
| Complexity | Simple | Complex |
| Example | RAG (Search â†’ Answer) | Research (Search â†’ Read â†’ Search Again â†’ Write) |

**Recommendation:** Start with Chains. Agents are hard to debug (LLM might do weird things).

---

## LangChain "Chain" Variations Demystified

LangChain can be overwhelming because the word "Chain" is used for three different things: **Basic Architectures**, **Document Strategies**, and **Legacy vs. Modern Syntax**.

### 1. The Core Architecture: Chains vs. Agents

This is the highest-level distinction. Do you want a fixed path or a dynamic brain?

| Variation | How it works | When to use |
|-----------|--------------|-------------|
| **Standard Chain** | **Hard-coded steps.** (A â†’ B â†’ C). The LLM is just a processor that follows your instructions exactly. | **90% of production apps.** Use this when you need reliability, speed, and predictable costs (e.g., RAG, Summarization, Extraction). |
| **Agent** | **Dynamic loop.** The LLM is given a "Goal" and "Tools" and decides *what* steps to take. It might loop 1 time or 10 times. | **Complex / Ambiguous tasks.** Use this when you don't know the steps in advance (e.g., "Research this company and write a report," where the LLM needs to search, read, search again). |

---

### 2. Document Processing Chains (The "Big Data" Problem)

If you build a RAG system (Chat with PDF), you will immediately face a problem: **"My PDF is too big for the prompt."**
LangChain provides 4 specific variations to solve this.

#### A. Stuff Chain (The Default)

- **How it works:** Takes *all* your documents, stuffs them into one prompt, and sends it to the LLM.
- **When to use:** **Always start here.** If your documents fit in the context window (e.g., 10 pages), this is the cheapest, fastest, and most accurate method because the LLM sees everything at once.

#### B. Map-Reduce Chain

- **How it works:**
  1. **Map:** Summarizes *each* page individually in parallel.
  2. **Reduce:** Takes those 100 summaries and summarizes them into one final answer.
- **When to use:** When you have **huge documents** (e.g., a whole book) and you need a summary of the *whole* thing. It is faster than Refine but can lose specific details during the "summary of a summary" steps.

#### C. Refine Chain

- **How it works:** Loops through pages one by one.
  - *Page 1:* "Here is the answer based on Page 1."
  - *Page 2:* "Update that answer using facts from Page 2."
- **When to use:** When you need **high accuracy** and detail across a large document. It is **slow** (cannot run in parallel) but produces the most comprehensive answers.

#### D. Map-Rerank Chain

- **How it works:** Asks the question to every single page individually and asks the LLM to give a "Confidence Score." It returns the answer with the highest score.
- **When to use:** When you are looking for a **single specific fact** hidden in a large pile of documents (e.g., "What is the fine for late payment?" inside a 300-page policy).

---

### 3. Syntax Variations: Legacy vs. LCEL

You will see two ways of writing LangChain code in tutorials. This is often the source of confusion.

- **Legacy Chains (`LLMChain`, `SequentialChain`):** The old way using Python classes. It is easier to read for beginners but harder to customize.
  ```python
  # Old Way
  chain = LLMChain(llm=llm, prompt=prompt)
  ```
- **LCEL (LangChain Expression Language):** The **modern standard**. It uses the "Pipe" `|` syntax. It is standard because it supports streaming and async out of the box.
  ```python
  # Modern Way
  chain = prompt | llm | output_parser
  ```
  **Advice:** If you are starting today, **learn LCEL**. Most new features (like LangGraph) rely on it.

### Summary Decision Matrix

| Your Goal | Recommended Variation |
|-----------|----------------------|
| **Simple Q&A** | `Stuff` Chain (written in LCEL) |
| **Summarize a Book** | `Map-Reduce` Chain |
| **Find a Needle in Haystack** | `Map-Rerank` Chain |
| **Complex Research** | `ReAct` Agent (or LangGraph) |
| **Chatbot with Memory** | `RunnableWithMessageHistory` (LCEL) |

---

### Deep Dive Resource

[Deep Dive into Map-Reduce, Stuff, and Refine Chains](https://www.youtube.com/watch?v=OTL4CvDFlro)

This video visually demonstrates the differences between the document processing chains (Stuff, Map-Reduce, Refine), which are often the most confusing part of LangChain for developers handling large data.

---

## LangGraph: Structured Agents

LangGraph bridges Chains and Agents. It's like **n8n** but for LLM workflows.

### The Key Difference

- **n8n:** "If `type == 'email'`, go to Gmail node"
- **LangGraph:** "Ask the LLM where to go next based on context"

### What Makes an Agent

The fundamental difference is **loops (cycles)**:
- **Chain:** A â†’ B â†’ C (one way)
- **Agent:** A â†’ B â†’ A â†’ B ... (loops until satisfied)

### LangGraph Example: Self-Correcting Writer

```
Research â†’ Write â†’ Check (LLM evaluates)
                     â”œâ”€ If Bad: Loop back to Research
                     â””â”€ If Good: Publish
```

**Benefits:**
- Reliability of defined process (Chain)
- Capability to self-correct (Agent)

---

## Agentic RAG Patterns

### 1. Routing RAG
Agent decides *where* to look:
- "Why is my code failing?" â†’ StackOverflow Vector DB
- "How do I request time off?" â†’ HR Notion Vector DB

### 2. Corrective RAG (CRAG)
Agent grades retrieval quality:
1. Retrieve documents
2. LLM evaluates: "Do these answer the question?"
3. If No â†’ Rewrite query, try again (or fallback to Google)
4. If Yes â†’ Generate answer

### 3. Self-RAG
Agent critiques its own answer:
- Generate answer
- Critic LLM checks: "Is this supported by documents?"
- If not, force rewrite

### Cost Comparison

| Feature | Standard RAG | Agentic RAG |
|---------|--------------|-------------|
| LLM Calls | 1 | 2+ |
| Speed | ~2s | ~10-30s |
| Reliability | Consistent | Variable |
| Accuracy | Good (simple queries) | Superior (complex queries) |

---

## Multi-Agent Architecture

### The "Brain": System Prompts

Different "agents" are the same LLM with different system prompts:

```python
# Agent A (Researcher)
system_prompt = "You are a Researcher. Use 'search_tool'. Do not write reports."

# Agent B (Writer)
system_prompt = "You are a Writer. Write blog posts from given facts. Do not search."
```

### The "Body": Execution Loop

```python
messages = [system_prompt, user_input]

while True:
    response = llm.chat(messages)

    if response.is_final_answer():
        return response.text

    elif response.wants_tool():
        tool_result = execute_tool(response.tool_name, response.tool_input)
        messages.append({"role": "tool_output", "content": tool_result})
        # Loop back - LLM reads tool output and thinks again
```

### Context Isolation

Prevent context pollution between agents:

1. **Researcher Agent:** Runs, builds huge context with search results
2. **Handoff:** Pass only final output, discard Researcher's context
3. **Writer Agent:** Starts fresh, only sees condensed facts

---

## Local vs Cloud: When to Use What

### Local Build (You are the Host)

**Pros:**
- Free (minus electricity)
- Private
- Full control

**Cons:**
- GPU wall: Need ~48GB VRAM for Llama-3-70B
- Concurrency: Sequential processing (User #5 waits)
- API key security risk

### AWS Bedrock (They are the Host)

**Pros:**
- Model access: Claude, Llama, Titan instantly
- Elastic scaling: 1 to 10,000 concurrent requests
- IAM security: No long-lived secrets
- Managed RAG: Point at S3, click "Sync"
- PrivateLink: Traffic never touches public internet

**Cons:**
- Pay per token
- Data leaves your machine

### When Local is Better

1. **Cost (small scale):** AWS managed vector stores have minimum monthly costs
2. **Debugging:** Easier to `print(context)` locally

### When Cloud is Better

1. **Enterprise security:** IAM, SOC2, HIPAA compliance
2. **Scale:** 100+ users, latency matters
3. **Maintenance:** Don't want to fix CUDA driver crashes at 3 AM

---

## The "Data Gravity" Problem

### Local Build Latency Issue

1. Retrieve 20 pages from local Vector DB
2. **Upload** all 20 pages to API over home internet
3. Result: Sluggish for large contexts

### Managed Cloud Solution

1. Documents live in AWS (S3)
2. Bedrock retrieves internally (gigabit speeds)
3. Only sends final answer back
4. Result: Much lower latency

---

## Why We Can't Just "Fix the DB"

### The Similarity vs Logic Problem

- **Vector DB:** Finds Document A ("Pro Plan costs $10") and Document B ("Enterprise costs $50")
- **The Question:** "Compare the pricing"
- **Vector DB Can't:** Synthesize "Enterprise is 5x more expensive"
- **LLM Required:** To read both and write the comparison

### The Chunking Paradox

| Chunk Size | Search Quality | Context Quality |
|------------|----------------|-----------------|
| Small | Great (specific) | Poor (loses context) |
| Large | Poor (muddy average) | Great (full context) |

**Solution:** Chains accept overhead to cheat this paradox.

### The "Bad User" Factor

- User: "it's not working"
- Best DB: Can't find "it" without context
- Solution: Query Transformation Chain rewrites to "Login Button failure"

---

## Advanced Indexing (Fixing the DB)

### Parent Document Retriever

1. **Index:** Tiny chunks (Sentence 1, Sentence 2...) - great for search
2. **Return:** Whole parent page - great for context
3. **Result:** Perfect search + Perfect context. Zero chain overhead.

### Hypothetical Questions (Reverse Indexing)

1. LLM pre-generates questions each paragraph answers
2. Index those questions
3. User question matches pre-generated question perfectly

---

## Summary: RAG Architecture Decisions

| Decision | Option A | Option B |
|----------|----------|----------|
| **Pipeline** | Standard RAG (fast, cheap) | Agentic RAG (flexible, expensive) |
| **Framework** | Raw Python (control) | LangChain (convenience) |
| **Agent Style** | Chains (predictable) | Agents (adaptive) |
| **Hosting** | Local (free, limited) | Cloud (paid, scalable) |
| **Indexing** | Basic chunks | Advanced (Parent Doc, Hypothetical Q) |

**Key Insight:** LLMs are processors, not storage. To let an LLM "read" your data, you must first build a search engine. That search engine IS RAG.

---

## A2A (Agent-to-Agent) Protocols

### Why the Hype?

The core concept is simple: one agent's output becomes another's input. The emphasis on A2A protocols (like Google's) comes from **standardization and interoperability**, not technical complexity.

### Problems A2A Tries to Solve

| Problem | Description |
|---------|-------------|
| **Discovery** | How does Agent A find Agent B that can help with a specific task? |
| **Capability Negotiation** | How do agents describe what they can do in a machine-readable way? |
| **Authentication & Trust** | When agents from different vendors talk, who vouches for whom? |
| **Structured Handoffs** | Raw text loses metadata (confidence scores, sources, partial results) |
| **Error Handling & Retries** | What happens when Agent B times out? |

### When Standards Matter

| Scenario | Standard Needed? |
|----------|------------------|
| Your own agents in one codebase | No - just function calls |
| Multi-vendor enterprise orchestration | Yes |
| Public agent marketplace | Yes |
| Local RAG + summarizer pipeline | No |

**Reality:** Much of the emphasis is marketing. For most developers, simple patterns work fine. A2A matters for the future vision of heterogeneous agent ecosystems across organizational boundaries - which doesn't exist at scale yet.

---

## Agent Communication Patterns

### 1. Function Calls

The simplest pattern - Agent A directly invokes Agent B as a function.

```python
# Agent B is just a function
def summarizer_agent(text: str) -> str:
    return llm.complete(f"Summarize this: {text}")

# Agent A calls it directly
def research_agent(topic: str) -> str:
    raw_data = search_web(topic)
    summary = summarizer_agent(raw_data)  # Direct call
    return f"Research findings: {summary}"
```

**Pros:** Simple, type-safe, debuggable
**Cons:** Tight coupling, must be same codebase/process

### 2. Message Passing

Agents communicate through a message broker/queue. Decoupled - agents don't know about each other directly.

```python
# Agent A publishes a task
message_queue.publish("summarization_tasks", {
    "request_id": "abc123",
    "text": raw_data,
    "reply_to": "research_agent_inbox"
})

# Agent B listens and responds
@message_queue.subscribe("summarization_tasks")
def summarizer_agent(message):
    summary = llm.complete(f"Summarize: {message['text']}")
    message_queue.publish(message["reply_to"], {
        "request_id": message["request_id"],
        "result": summary
    })
```

**Pros:** Loose coupling, scalable, agents can be different languages/processes
**Cons:** More infrastructure, async complexity, harder to debug

### 3. Shared Context

Agents read/write to a shared state store. No direct communication - coordinate through data.

```python
# Shared state (Redis, database, or in-memory)
context = SharedContext()

# Agent A writes
def research_agent(topic: str):
    context.set("raw_data", search_web(topic))
    context.set("status", "needs_summarization")

# Agent B polls and reacts
def summarizer_agent():
    if context.get("status") == "needs_summarization":
        raw = context.get("raw_data")
        context.set("summary", llm.complete(f"Summarize: {raw}"))
        context.set("status", "done")
```

**Pros:** Simple coordination, easy to inspect state, good for workflows
**Cons:** Race conditions possible, polling inefficiency, implicit dependencies

### Pattern Comparison

| Scale | Common Choice |
|-------|---------------|
| Single app, few agents | Function calls |
| Microservices, team boundaries | Message passing (Redis, RabbitMQ, Kafka) |
| Complex workflows with state | LangGraph, Temporal, or shared context |
| Cross-organization | Not common yet - A2A is early |

---

## Industry Solutions to A2A Problems

| Problem | Solutions |
|---------|-----------|
| **Discovery** | Google A2A Agent Cards (JSON at `/.well-known/agent.json`), Microsoft Copilot Plugin manifests |
| **Capability Negotiation** | OpenAPI/JSON Schema, A2A Agent Cards with `skills` array, MCP (Anthropic) |
| **Authentication & Trust** | OAuth 2.0 / OpenID Connect, mTLS, Azure AD/Okta federation |
| **Structured Handoffs** | A2A Task Protocol (states: pending, running, completed, failed), LangGraph checkpoints, Temporal workflows |
| **Error Handling** | A2A task states, message queue dead letter queues, Temporal retry policies, circuit breakers |

### A2A Agent Card Example

```json
// Hosted at https://travel-agent.example/.well-known/agent.json
{
  "name": "Travel Booking Agent",
  "description": "Books flights and hotels",
  "url": "https://travel-agent.example/a2a",
  "skills": [
    {
      "id": "book_flight",
      "name": "Book Flight",
      "inputSchema": {
        "type": "object",
        "properties": {
          "origin": {"type": "string"},
          "destination": {"type": "string"},
          "date": {"type": "string", "format": "date"}
        }
      }
    }
  ],
  "authentication": {
    "schemes": ["bearer"]
  }
}
```

---

## Message Brokers (Kafka, RabbitMQ, etc.)

### How It Works

A message broker is a **separate running process** (server) that agents connect to. Think of it as a post office:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent A â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚   Broker     â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚ Agent B â”‚
â”‚ (Python)â”‚  write  â”‚ (Kafka/Redis)â”‚  read   â”‚ (Java)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    persistent storage
```

### Connection Protocol

Not HTTP - typically **TCP with custom binary protocol** for performance:

| Broker | Protocol |
|--------|----------|
| Kafka | Custom binary over TCP (port 9092) |
| RabbitMQ | AMQP over TCP |
| Redis Pub/Sub | Redis protocol (RESP) over TCP |
| AWS SQS | HTTPS (managed service exception) |

### Why Different Codebases Can Communicate

They all speak to the **same broker** using language-specific **client libraries**:

```python
# Agent A - Python codebase
from kafka import KafkaProducer
producer = KafkaProducer(bootstrap_servers='kafka-server:9092')
producer.send('tasks', b'{"job": "summarize", "text": "..."}')
```

```java
// Agent B - Java codebase (completely separate repo)
KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
consumer.subscribe(Arrays.asList("tasks"));
// receives: {"job": "summarize", "text": "..."}
```

### Mental Model

```
Database:    App A writes row  â†’ PostgreSQL stores â†’ App B reads row
Broker:      Agent A sends msg â†’ Kafka stores      â†’ Agent B receives msg
```

The difference: brokers are optimized for **streaming/queuing** rather than querying.

---

## Shared Context vs Zustand (Frontend)

Very similar concept - both are "central store that multiple consumers read/write."

| Aspect | Zustand (Frontend) | Shared Context (Agents) |
|--------|-------------------|------------------------|
| Storage | In-memory JS object | Redis, database, or memory |
| Subscribers | React components | Agent processes |
| Updates | `set()` triggers re-render | Polling or pub/sub notification |
| Scope | Single browser tab | Across processes/machines |

### Key Difference

Zustand is **reactive** (automatic re-render on change). Agent shared context typically needs explicit:
- **Polling:** Check every N seconds
- **Pub/Sub:** Separate notification channel

```python
# Redis example with pub/sub notification
def research_agent():
    r.set("raw_data", json.dumps(fetched_data))
    r.publish("updates", "raw_data_ready")  # notify

def summarizer_agent():
    pubsub = r.pubsub()
    pubsub.subscribe("updates")
    for message in pubsub.listen():
        if message["data"] == b"raw_data_ready":
            raw = json.loads(r.get("raw_data"))
            # process it
```

---

## AWS Bedrock Knowledge Bases: Reality Check

### What Bedrock Automates

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Bucket â”‚â”€â”€â”€â”€â–ºâ”‚           Bedrock Knowledge Base            â”‚
â”‚  (your docs)â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚ Chunkingâ”‚â”€â–ºâ”‚ Embed   â”‚â”€â–ºâ”‚ Vector Storeâ”‚  â”‚
                    â”‚  â”‚ (auto)  â”‚  â”‚ (Titan) â”‚  â”‚ (OpenSearch)â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Step | Bedrock Handles It? |
|------|---------------------|
| Document parsing (PDF, DOCX) | âœ… Yes |
| Text chunking | âš ï¸ Partially (limited config) |
| Embedding | âœ… Yes (Titan or Cohere) |
| Vector storage | âœ… Yes (managed OpenSearch/Aurora) |
| Retrieval | âœ… Yes |
| Prompt + LLM | âœ… Yes (RetrieveAndGenerate) |

### What You Still Must Do

1. **Infrastructure Setup**
   - Create S3 bucket with proper structure
   - Create IAM roles (Bedrock â†’ S3, Bedrock â†’ Vector DB)
   - Choose/provision vector store
   - Create Knowledge Base
   - Trigger sync (manual or scheduled)

2. **Chunking Configuration** (limited options)
   ```python
   chunking_config = {
       "chunkingStrategy": "FIXED_SIZE",  # or HIERARCHICAL, SEMANTIC
       "fixedSizeChunkingConfiguration": {
           "maxTokens": 300,
           "overlapPercentage": 20
       }
   }
   ```

3. **Metadata & Filtering**
   ```json
   // S3: documents/legal/contract.pdf.metadata.json
   {
       "metadataAttributes": {
           "department": "legal",
           "year": 2024
       }
   }
   ```

### Two Ways to Use Bedrock

**Option A: Full Managed (Simple, Less Control)**
```python
response = bedrock.retrieve_and_generate(
    input={"text": "What is our refund policy?"},
    retrieveAndGenerateConfiguration={
        "type": "KNOWLEDGE_BASE",
        "knowledgeBaseConfiguration": {
            "knowledgeBaseId": "KB123ABC",
            "modelArn": "arn:aws:bedrock:...anthropic.claude-3-sonnet"
        }
    }
)
```

**Option B: Retrieve Only (More Control)**
```python
# Step 1: Just get chunks
chunks = bedrock.retrieve(knowledgeBaseId="KB123", ...)

# Step 2: Build your own prompt
prompt = f"Based on: {chunks}\n\nAnswer: ..."

# Step 3: Call LLM yourself
response = bedrock_runtime.invoke_model(...)
```

### What Bedrock Doesn't Solve

| Challenge | Still Your Problem |
|-----------|-------------------|
| Bad source documents | Garbage in â†’ garbage out |
| Complex tables/charts | PDF parsing is imperfect |
| Evaluation | No built-in metrics |
| Hallucination prevention | Need prompt engineering |
| Real-time data | Must re-sync; not live |

---

## Infrastructure as Code: Console vs CDK vs Terraform

Three ways to create AWS resources - same end result, different methods.

### Console (AWS Web UI)

Click-through in browser. Good for learning, bad for production.

### CDK (AWS Cloud Development Kit)

Define infrastructure in **real code** (TypeScript, Python):

```typescript
const kb = new bedrock.CfnKnowledgeBase(this, 'MyKnowledgeBase', {
  name: 'my-kb',
  roleArn: role.roleArn,
  knowledgeBaseConfiguration: {
    type: 'VECTOR',
    vectorKnowledgeBaseConfiguration: {
      embeddingModelArn: 'arn:aws:bedrock:...amazon.titan-embed-text-v2',
    },
  },
  // ...
});
```

Deploy: `cdk deploy`

### Terraform (HashiCorp)

Define infrastructure in **HCL**:

```hcl
resource "aws_bedrockagent_knowledge_base" "main" {
  name     = "my-kb"
  role_arn = aws_iam_role.bedrock.arn
  # ...
}
```

Deploy: `terraform apply`

### Comparison

| Aspect | Console | CDK | Terraform |
|--------|---------|-----|-----------|
| Learning curve | Low | Medium | Medium |
| Version control | âŒ | âœ… Git | âœ… Git |
| Reproducible | âŒ | âœ… | âœ… |
| Multi-cloud | âŒ | âŒ | âœ… |

### The Key Idea

```
Console = imperative, manual
   "Click this, then click that..."

CDK/Terraform = declarative, automated
   "Here's what I want. You figure out how to create it."
```

With IaC:
- New team member â†’ `git clone` + `terraform apply` â†’ identical environment
- Something breaks â†’ `git revert` + redeploy â†’ back to working state
- Audit â†’ git history shows who changed what, when

---

## Implementation: Line-Level Citation System

Added source citation support to the RAG implementation so the LLM can cite which file and line numbers information came from.

### Changes Made to `main.py`

#### 1. Custom Text Loader with Line Tracking

Created `LineTrackingTextLoader` class that stores line offset mappings when loading documents:

```python
class LineTrackingTextLoader(BaseLoader):
    """Text loader that tracks line numbers for citation support."""

    def lazy_load(self) -> Iterator[Document]:
        # Build line offset index: list of (start_char, end_char) for each line
        line_offsets = []
        current_pos = 0
        for line in content.split("\n"):
            line_end = current_pos + len(line)
            line_offsets.append((current_pos, line_end))
            current_pos = line_end + 1

        metadata = {
            "source": str(self.file_path),
            "line_offsets": line_offsets,
            "total_lines": len(line_offsets),
        }
        yield Document(page_content=content, metadata=metadata)
```

#### 2. Line Number Calculation

Added `calculate_line_numbers()` function that converts character positions to line numbers after chunking:

```python
def calculate_line_numbers(chunks: list[Document]) -> list[Document]:
    for chunk in chunks:
        start_char = chunk.metadata["start_index"]
        end_char = start_char + len(chunk.page_content)
        # Find which lines these character positions correspond to
        chunk.metadata["start_line"] = ...
        chunk.metadata["end_line"] = ...
        # Clean up large metadata before storing in vector DB
        del chunk.metadata["line_offsets"]
    return chunks
```

#### 3. Enable Position Tracking in Splitter

Modified `split_documents()` to use `add_start_index=True`:

```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    add_start_index=True,  # Track character position for line calculation
)
```

#### 4. Citation-Aware Document Formatting

Created `format_docs_with_citations()` to include source metadata in context:

```python
def format_docs_with_citations(docs: list[Document]) -> str:
    formatted = []
    for doc in docs:
        filename = Path(doc.metadata["source"]).name
        header = f"[Source: {filename}, lines {start_line}-{end_line}]"
        formatted.append(f"{header}\n{doc.page_content}")
    return "\n\n---\n\n".join(formatted)
```

#### 5. Updated Prompt Template

Modified the prompt to instruct the LLM to cite sources:

```python
template = """Answer the question based on the following context.
Each context chunk shows its source file and line numbers in brackets.
When you use information from the context, cite the source using the format [filename:lines].
...
"""
```

### Bug Fix: Windows File Lock on Rebuild

**Problem:** `shutil.rmtree()` failed with `PermissionError` when trying to delete `chroma_db` folder because Chroma's SQLite connection held a file lock.

**Solution:** Instead of deleting the folder, use Chroma's `delete_collection()` method:

```python
if question.lower() == "rebuild":
    # Delete existing collection (avoids Windows file lock issues with rmtree)
    try:
        vectorstore._client.delete_collection(vectorstore._collection.name)
    except Exception:
        pass  # Collection might not exist
    vectorstore = get_or_create_vectorstore(chunks, embeddings)
```

### UX Improvement: Loading Message

Added a progress message before heavy library imports to explain the startup delay:

```python
print("Starting RAG Practice...", flush=True)
print("Loading libraries (this may take a moment)...", flush=True)
```

The delay is caused by importing heavy libraries (PyTorch, transformers, chromadb), not by the embedding model loading.

### Limitations

1. **Line numbers are approximate** - chunks may span partial lines due to character-based splitting
2. **LLM citation accuracy** - even with metadata provided, LLMs can occasionally misattribute or hallucinate citations
3. **Overlap complication** - with 200-char overlap, same content appears in multiple chunks with different line ranges
4. **Vectorstore rebuild required** - existing vectorstore lacks line metadata; must rebuild after updating code

### Usage

After updating the code, rebuild the vectorstore to include line metadata:

```
You: rebuild
Rebuilding vectorstore...
Done! Vectorstore rebuilt.

You: What is RAG?
Assistant: RAG (Retrieval-Augmented Generation) is a technique that combines... [sample.txt:1-15]
```

---

---

## Web Application Implementation

Converted the CLI RAG application to a web-based system for demo purposes (ClassMethod recruiter demo).

### Architecture Decision: Monorepo Structure

```
simple-rag-app/
â”œâ”€â”€ frontend/           # Next.js (Vercel)
â”‚   â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ lib/
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ backend/            # FastAPI (Hugging Face Spaces)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ pyproject.toml
â””â”€â”€ dev-notes/
```

**Why Monorepo:** Keeps related code together, easier to maintain for a demo project. Clear separation between frontend (TypeScript) and backend (Python).

---

## Hosting Platform Comparison

### Why Vercel Cannot Host Python ML Backend

| Limitation | Impact |
|------------|--------|
| **250MB function size** | Embedding model alone is ~500MB+ |
| **10s cold start timeout** | Model loading takes 30-60s |
| **No GPU support** | Inference on CPU only (slow) |
| **Serverless architecture** | No persistent model in memory |

**Verdict:** Vercel is excellent for Next.js frontend, but cannot run ML workloads.

### Railway vs Hugging Face Spaces

| Feature | Railway | Hugging Face Spaces |
|---------|---------|---------------------|
| **Free tier** | $5/month credit | Free CPU tier |
| **Model caching** | Manual (volume mounts) | Automatic (`HF_HOME`) |
| **ML optimization** | Generic container | ML-optimized images |
| **Cold start** | Container restart = full reload | Pre-cached models |
| **Community** | General dev | ML-focused |
| **Sleep policy** | Pay to stay awake | 48hr inactive â†’ sleep |

**Decision:** Hugging Face Spaces - better for ML workloads, automatic model caching, free.

### Hugging Face Spaces Configuration

```yaml
# README.md (yaml frontmatter)
---
title: RAG Demo API
emoji: ğŸ”
sdk: docker
app_port: 7860
---
```

**Port 7860** is the default for HF Spaces. Health checks hit root path.

---

## Embedding Model Selection

### Options Considered

| Model | Size | Japanese Quality |
|-------|------|------------------|
| `paraphrase-multilingual-MiniLM-L12-v2` | 118MB | Good |
| `intfloat/multilingual-e5-large` | 560MB | **Excellent** |

**Decision:** `multilingual-e5-large` - significantly better Japanese semantic understanding despite larger size.

### Model Caching on HF Spaces

```python
# HF Spaces automatically sets HF_HOME
# Models cached in /data/.cache/huggingface/

# First request: Downloads model (~30-60s)
# Subsequent requests: Instant load from cache
# After sleep (48hr): Cache persists, fast wake
```

**Key insight:** Unlike Railway where you need explicit volume mounts, HF Spaces handles model caching automatically.

---

## Cold Start UX Handling

### The Problem

1. User visits app
2. Frontend loads instantly (Vercel)
3. First API call triggers backend wake-up
4. User stares at frozen screen for 30-60s

### The Solution

Created `ServerStartingOverlay` component with health check polling:

```typescript
// useServerStatus.ts
export function useServerStatus() {
  const checkServerHealth = useCallback(async () => {
    try {
      const health = await checkHealth();
      if (health.model_loaded && health.vectorstore_ready) {
        setStatus("ready");
      } else {
        setStatus("starting");
        setTimeout(() => setRetryCount((c) => c + 1), 3000);
      }
    } catch {
      setStatus("starting");
      setTimeout(() => setRetryCount((c) => c + 1), 5000);
    }
  }, []);
}
```

**UX Pattern:**
- Show loading overlay immediately
- Poll health endpoint every 3-5 seconds
- Display Japanese message explaining the wait
- Remove overlay when `model_loaded && vectorstore_ready`

### Health Check Endpoint

```python
@router.get("")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": embedding_service.is_loaded(),
        "vectorstore_ready": vectorstore_service.is_ready(),
    }
```

---

## Streaming Implementation (SSE)

### Why SSE Over WebSocket

| Feature | SSE | WebSocket |
|---------|-----|-----------|
| Direction | Server â†’ Client only | Bidirectional |
| Complexity | Simple HTTP | Connection management |
| Reconnection | Automatic | Manual |
| Use case | Streaming text | Chat apps, games |

**Decision:** SSE - simpler, sufficient for streaming LLM responses.

### Backend (FastAPI)

```python
@router.post("")
async def chat(request: ChatRequest):
    async def generate():
        async for event in rag_service.stream_query(message, history):
            event_type = event.get("type", "token")
            event_data = json.dumps(event.get("data", {}), ensure_ascii=False)
            yield f"event: {event_type}\ndata: {event_data}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")
```

### Frontend (TypeScript)

```typescript
export async function* streamChat(message: string, history: Message[]) {
  const response = await fetch(`${baseUrl}/api/chat`, {
    method: "POST",
    body: JSON.stringify({ message, history }),
  });

  const reader = response.body?.getReader();
  const decoder = new TextDecoder();
  let buffer = "";

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    buffer += decoder.decode(value, { stream: true });
    // Parse SSE events from buffer...
  }
}
```

### Event Types

| Event | Data | Purpose |
|-------|------|---------|
| `sources` | `{sources: [...]}` | Citation metadata |
| `token` | `{content: "..."}` | Streaming text chunk |
| `done` | `{}` | Stream complete |
| `error` | `{message: "..."}` | Error occurred |

---

## UI/UX Decisions

### Japanese-First Design

- All UI text in Japanese (constants.ts)
- Font: Noto Sans JP (Google Fonts)
- Sample documents in Japanese (fictional company)

### Component Architecture

```
ChatInterface
â”œâ”€â”€ ServerStartingOverlay (cold start)
â”œâ”€â”€ Header (title + document button)
â”œâ”€â”€ MessageList
â”‚   â””â”€â”€ MessageBubble (streaming support)
â”‚       â””â”€â”€ SourceCitation (clickable)
â”œâ”€â”€ ChatInput (auto-resize)
â””â”€â”€ DocumentDrawer (slide-in panel)
    â”œâ”€â”€ DocumentUpload (drag & drop)
    â””â”€â”€ DocumentList
```

### Drawer vs Modal for Documents

**Decision:** Drawer - less intrusive, can reference chat while managing documents.

---

## Rate Limiting (Gemini Free Tier)

```python
# 15 requests per minute for Gemini API
class RateLimiter:
    def __init__(self, requests_per_minute: int = 15):
        self.window = 60
        self.max_requests = requests_per_minute
        self.requests = []

    async def acquire(self):
        # Sliding window implementation
        ...
```

**Frontend handling:** Show Japanese error message, suggest waiting.

---

## CORS Configuration

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",           # Local dev
        "https://*.vercel.app",            # Vercel preview
        "https://your-domain.vercel.app",  # Production
    ],
    allow_methods=["*"],
    allow_headers=["*"],
)
```

---

## Files Created

### Backend
- `backend/app/main.py` - FastAPI entry, lifespan management
- `backend/app/config.py` - Pydantic settings
- `backend/app/routers/chat.py` - SSE streaming endpoint
- `backend/app/routers/documents.py` - CRUD operations
- `backend/app/routers/health.py` - Health check
- `backend/app/services/rag_service.py` - Core RAG logic
- `backend/app/services/embedding_service.py` - Singleton model
- `backend/app/services/document_service.py` - File handling
- `backend/app/services/vectorstore_service.py` - Chroma operations
- `backend/app/utils/rate_limiter.py` - Gemini rate limiting
- `backend/app/models/schemas.py` - Pydantic schemas
- `backend/Dockerfile` - HF Spaces deployment
- `backend/app/data/sample_documents/*.txt` - Japanese sample docs

### Frontend
- `frontend/app/components/ChatInterface.tsx` - Main container
- `frontend/app/components/MessageBubble.tsx` - Message display
- `frontend/app/components/ChatInput.tsx` - Auto-resize input
- `frontend/app/components/SourceCitation.tsx` - Citations
- `frontend/app/components/DocumentDrawer.tsx` - Document panel
- `frontend/app/components/ServerStartingOverlay.tsx` - Cold start UI
- `frontend/lib/api.ts` - Backend client + SSE
- `frontend/lib/types.ts` - TypeScript interfaces
- `frontend/lib/constants.ts` - Japanese UI text
- `frontend/hooks/useChat.ts` - Chat state management
- `frontend/hooks/useDocuments.ts` - Document CRUD
- `frontend/hooks/useServerStatus.ts` - Health polling

---

## Key Learnings

1. **Vercel + Python ML = No** - Vercel is for frontend/serverless, not ML inference
2. **HF Spaces model caching is automatic** - Unlike Railway, no volume mount needed
3. **SSE > WebSocket for LLM streaming** - Simpler, sufficient for unidirectional
4. **Cold start UX matters** - Users need feedback during 30-60s waits
5. **multilingual-e5-large >> MiniLM for Japanese** - Worth the size tradeoff

---

## Vercel Deployment - The GitIgnore Root Cause

### The Problem

Vercel build kept failing with module resolution errors:

```
Module not found: Can't resolve '@/lib/api'
Module not found: Can't resolve '@/lib/constants'
```

Interestingly, `@/lib/types` worked (type-only import) but `@/lib/api` and `@/lib/constants` failed (value imports).

### Investigation Timeline

1. **Attempt 1-6:** Tried various fixes:
   - Added `baseUrl` to tsconfig.json
   - Added `turbopack.resolveAlias` to next.config.ts
   - Tried `--webpack` flag (caused EISDIR error on Windows)
   - Converted path aliases to relative imports

2. **The Real Root Cause:** Opened `.gitignore` and found line 50:
   ```
   lib/
   ```
   This Python packaging convention was matching `frontend/lib/`!

### Why It Worked Locally But Failed on Vercel

- **Local:** Files existed in the working directory
- **Vercel:** Clones from GitHub where `frontend/lib/` was never pushed (ignored)
- **Type imports worked:** `import type {...}` is erased at compile time, doesn't need actual file

### The Fix

1. **Clean up `.gitignore`** - Removed 14 legacy Python entries:
   - `lib/` â† **THE CULPRIT**
   - `*$py.class`, `.Python`, `develop-eggs/`, `downloads/`, `eggs/`, `.eggs/`
   - `lib64/`, `parts/`, `sdist/`, `var/`, `wheels/`, `.installed.cfg`, `*.egg`, `ENV/`

2. **Keep only modern Python entries:**
   ```gitignore
   __pycache__/
   *.py[cod]
   *.so
   build/
   dist/
   *.egg-info/
   .venv
   venv/
   ```

3. **Force add** `frontend/lib/` to git tracking

### Lesson Learned

**Always check `.gitignore` when files work locally but "don't exist" in CI/CD!**

The legacy Python gitignore template (from GitHub's default) includes `lib/` for old-style package layouts, but this pattern matches any `lib/` folder anywhere in the repo - including `frontend/lib/`.

Modern Python tooling (uv, poetry, hatchling) doesn't use `lib/` - it's safe to remove.

---

## Next Steps

- [ ] Implement Parent Document Retriever
- [ ] Try Corrective RAG pattern
- [ ] Experiment with LangGraph for self-correcting workflows
- [ ] Compare latency: local vs API-based retrieval
- [ ] Set up a simple message broker (Redis Pub/Sub) for agent communication
- [ ] Try AWS Bedrock Knowledge Base with sample documents
- [ ] Deploy backend to Hugging Face Spaces
- [ ] Deploy frontend to Vercel
- [ ] Test end-to-end cold start experience
- [ ] Add document upload functionality testing
