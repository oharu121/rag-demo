# Development Notes - 2025-12-16

## Feature: Evaluation Panel & DatasetSelector Redesign

### Overview
Implemented a comprehensive evaluation UI to demonstrate RAG accuracy differences between the original and optimized datasets. The key insight this feature showcases: **データの品質と検索精度がLLMの最適化より重要** (data quality and retrieval precision are more important than LLM optimization).

### Branch
`feature/evaluation-panel`

---

## Implementation Summary

### Part 1: DatasetSelector with Radio Cards
**File: `frontend/app/components/DatasetSelector.tsx`**

Replaced the dropdown selectors with descriptive radio cards that explain the problem/solution for each dataset:

- **元データ (Original)**: "例外規定が附則に埋もれており、検索で見逃されやすい"
- **最適化済み (Optimized)**: "正社員・パート・アルバイト別に規定を分割し、正確に検索可能"

Features:
- Radio card design with visual selection indicator
- Collapsible "詳細設定" section for chunking strategy (hidden by default)
- Visual feedback with emerald theme for optimized selection

### Part 2: In-Chat Evaluation with Scoring
Natural chat flow for evaluation - tests appear as regular Q&A messages with inline scoring annotations.

**New Components:**
- `frontend/app/components/ScoringAnnotation.tsx` - Displays ✅/❌ with explanation
- `frontend/app/components/EvaluationSummary.tsx` - Results summary bar with progress visualization

**Modified Files:**
- `frontend/hooks/useChat.ts` - Added `scoring?: ScoringData` to messages, `addEvaluationMessage()` function
- `frontend/app/components/MessageBubble.tsx` - Renders ScoringAnnotation when scoring data present
- `frontend/app/components/ChatInterface.tsx` - Added evaluation button, progress indicator, summary display

**Key Behaviors:**
- "精度テスト" button runs 4 test queries sequentially
- Each Q&A appears in chat with ✅/❌ annotation
- Summary bar appears at bottom after completion
- Chat clears when switching datasets (fresh comparison)

### Part 3: Backend Evaluation Endpoint
**File: `backend/app/routers/evaluation.py`**

New API endpoint `POST /api/evaluate/quick` that:
1. Loads test queries from `evaluation/test_queries_light.json`
2. Runs each query against the RAG system
3. Scores answers using `check_answer_quality()` function
4. Returns structured results with scoring for each query

**Light Test Suite (4 queries):**
Selected for maximum impact - 2 general (baseline), 2 exception (key differentiator):

| ID | Type | Question | Purpose |
|----|------|----------|---------|
| general_02 | baseline | 通勤手当の上限額は？ | Both should pass (50,000円) |
| general_03 | baseline | 年次有給休暇の付与時期は？ | Both should pass (6ヶ月後10日) |
| exception_02 | differentiator | アルバイトの通勤手当上限は？ | Original fails (says 50,000), Optimized passes (20,000) |
| exception_03 | differentiator | アルバイトは結婚休暇取得可能？ | Original may fail, Optimized passes (対象外) |

### Part 4: Integration & Theme Styling

**Visual Theme Changes:**
- Background shifts to emerald/green tint when "最適化済み" selected
- Smooth 500ms transition between themes
- CSS classes added to `frontend/app/globals.css`

**ChatInterface Updates:**
- Dynamic background color based on dataset
- Evaluation button with loading spinner and progress (テスト中 X/4)
- Input disabled during evaluation
- EvaluationSummary displayed after tests complete

---

## Files Created/Modified

### New Files
| File | Purpose |
|------|---------|
| `frontend/app/components/DatasetSelector.tsx` | Radio card selector with descriptions |
| `frontend/app/components/ScoringAnnotation.tsx` | ✅/❌ annotation for scored messages |
| `frontend/app/components/EvaluationSummary.tsx` | Results summary with progress bar |
| `backend/app/routers/evaluation.py` | Evaluation API endpoint |
| `evaluation/test_queries_light.json` | 4-query light test suite |

### Modified Files
| File | Changes |
|------|---------|
| `frontend/app/components/ChatInterface.tsx` | Evaluation flow, theme switching, dataset change handling |
| `frontend/app/components/MessageBubble.tsx` | ScoringAnnotation rendering |
| `frontend/hooks/useChat.ts` | Scoring data in messages, addEvaluationMessage() |
| `frontend/lib/types.ts` | Evaluation types (ScoringData, EvaluationResponse, etc.) |
| `frontend/lib/api.ts` | runEvaluation(), fetchTestQueries() |
| `frontend/app/globals.css` | Theme CSS classes |
| `backend/app/main.py` | Registered evaluation router |

### Deleted Files
| File | Reason |
|------|--------|
| `frontend/app/components/StrategySelector.tsx` | Replaced by DatasetSelector |

---

## Expected Demo Flow

1. User opens app → sees DatasetSelector with "元データ" selected by default
2. User clicks "精度テスト" → chat clears, tests run with progress indicator
3. Results appear as natural Q&A with ✅/❌ annotations
4. Summary shows (e.g., 2/4 = 50% for original)
5. User switches to "最適化済み" → background shifts to emerald, chat clears
6. User runs test again → expects 4/4 = 100%
7. User can now explain why optimized data matters more than LLM tuning

---

## Technical Notes

- Evaluation runs synchronously on backend (not streaming) for simplicity
- Frontend adds 300ms delay between messages for visual effect
- Scoring logic checks for required terms AND prohibited terms (catches wrong employee type answers)
- Test queries chosen to highlight the "exception buried in appendix" problem
