# Dev Notes - 2025-12-18

## Problem: Evaluation Tests Too Easy

### What We Observed
- Raw dataset + simple chunking strategy passed 90%+ of evaluation tests
- This undermines the demo's goal: proving that **data quality matters more than LLM optimization**
- Initial test set (`test_queries_light.json`) had only 4 queries - too few to differentiate

### Root Cause Analysis
1. **General queries are too easy** - Questions like "通勤手当の上限額は？" have answers in plain sight in the documents
2. **Exception cases are the real differentiator** - Questions about part-timers (アルバイト, パート) require finding 附則 (supplementary provisions) which are harder to retrieve
3. **Term matching was too strict** - Answers using different but correct phrasing (e.g., "認められていません" vs "できない") were marked as failures

### What We Changed

#### 1. Created Hard Test File (`test_queries_hard.json`)
- **10 exception-only queries** - all test edge cases for part-timers
- Topics covered: 通勤手当, 休暇, 経費, リモートワーク, 福利厚生, 服務
- Each query tests whether RAG can find:
  - Different rules for アルバイト vs 正社員
  - Specific limits (e.g., 2万円 vs 5万円)
  - Prohibited actions for certain employee types

#### 2. Added More Expected Term Variations
Before:
```json
"expected_answer_contains": ["認められない", "できない", "対象外"]
```

After:
```json
"expected_answer_contains": [
  "認められない", "できない", "対象外",
  "認められていません", "できません", "取得できません",
  "適用されません", "許可されていません"
]
```

#### 3. Improved Failure Feedback (`ScoringAnnotation.tsx`)
- Shows expected terms when test fails: `期待: 「term1」「term2」「term3」など`
- Shows prohibited terms if incorrectly found: `誤検出: term1, term2`
- Helps debug why tests fail

#### 4. Simplified Test Selection
- Removed test difficulty selector from UI
- Always runs hard test - simpler UX, clearer differentiation

### Files Modified
- `backend/app/config.py` - Changed to use `test_queries_hard.json`
- `backend/app/data/evaluation/test_queries_hard.json` - NEW: 10 exception queries
- `backend/app/data/evaluation/test_queries_medium.json` - Added term variations
- `frontend/app/components/ScoringAnnotation.tsx` - Shows expected terms on failure

### Expected Outcome
With harder exception-only queries:
- **Raw dataset**: Should score lower (附則 scattered, harder to retrieve)
- **Optimized dataset**: Should score higher (exceptions better structured)

This better demonstrates that data quality and structure matter more than chunking strategy alone.

---

## Streaming Evaluation Implementation

### Problem
- User had to wait 30-60 seconds for all results to appear at once
- Poor UX - no feedback during evaluation

### Solution
Implemented token-by-token streaming for evaluation (like regular chat):

#### Backend (`evaluation.py`)
- New SSE endpoint: `GET /api/evaluate/stream`
- Event types:
  - `query_start` - question about to be asked
  - `token` - answer token (streamed)
  - `query_done` - scoring for completed query
  - `complete` - final score summary

#### Frontend
- `api.ts`: Added `streamEvaluation()` async generator
- `useChat.ts`: Added streaming evaluation methods
- `ChatInterface.tsx`: Handles streaming events, shows progress

### Result
- Questions and answers appear one by one as they're processed
- Real-time token streaming for each answer
- Progress indicator shows "Testing 3/10..."
- Much better UX

---

## Evaluation Logs Not Showing in Hugging Face Spaces

### Problem
After deploying to HF Spaces, evaluation logs were not appearing in the container logs. Only HTTP request logs were visible:
```
INFO:     10.16.9.169:23145 - "GET /api/evaluate/stream?document_set=original&strategy=standard HTTP/1.1" 200 OK
```

No details about which questions were being asked, what answers were generated, or scoring results.

### Root Cause
Python's `logging.getLogger(__name__)` creates a logger but **doesn't attach handlers by default**. In containerized environments like Hugging Face Spaces, only `print()` statements with `flush=True` go to the visible container logs.

The code had:
```python
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Later in code...
logger.info(f"[Evaluation Stream] Q: {question}")  # NOT visible in HF logs!
```

The `/quick` endpoint had both `logger.info()` AND `print()` statements (so it worked), but the `/stream` endpoint only had `logger.info()` calls.

### Fix
Changed all `logger.info()` / `logger.error()` calls in the streaming endpoint to `print(..., flush=True)`:

```python
# Before (not visible in HF Spaces)
logger.info(f"[Evaluation Stream] Q: {question}")

# After (visible in HF Spaces)
print(f"[Eval {index+1}/{total}] Q: {question}", flush=True)
```

### Log Output Format
After the fix, logs now show:
```
[Eval] Starting evaluation: document_set=original, strategy=standard
[Eval] Loaded 10 test queries
[Eval 1/10] Q: アルバイトの通勤手当の申請方法を教えてください。
[Eval 1/10] A: アルバイトの通勤手当は紙の「通勤届（短期雇用者用）」を...
[Eval 1/10] ✓ found=['紙', '店舗責任者'] | missing=[] | prohibited=[]
[Eval 2/10] Q: アルバイトの通勤手当の上限額はいくらですか？
...
[Eval] Complete: 7/10 (70%)
```

### Key Takeaway
**In Docker/containerized environments, always use `print(..., flush=True)` for logs that need to be visible.** Python's logging module requires explicit handler configuration to output to stdout, which may not be set up by default in container environments.

### File Modified
- `backend/app/routers/evaluation.py` - Changed `logger.info()` to `print(..., flush=True)`

---

## Research: Real-World RAG Failure Scenarios

### Why Current Tests Are Too Easy

Our current documents are **too well-structured** for realistic RAG testing:

1. **Exceptions are explicitly labeled** - Headers like "附則第3条（アルバイト・臨時スタッフに関する特則）"
2. **Topic keywords are repeated** - Exception sections re-state "通勤手当", "申請方法" etc.
3. **Documents are small** - Even naive chunking captures related information
4. **Strong keyword match** - Query "アルバイトの通勤手当" easily matches chunk with both keywords

The RAG succeeds because semantic similarity is high between query and exception chunk.

### Real RAG Failure Scenarios (from Research)

Based on research from [IBM](https://www.ibm.com/think/insights/rag-problems-five-ways-to-fix), [LAC Watch](https://www.lac.co.jp/lacwatch/people/20240118_003651.html), [Enterprise RAG Challenge](https://github.com/trustbit/enterprise-rag-challenge), and [生成AI社内活用ナビ](https://officebot.jp/columns/basic-knowledge/rag-assignment/):

#### 1. Distant Chunk with Implicit Reference (課題シナリオの核心)
- Long irrelevant text separates general rule from exception
- Exception doesn't re-state the topic ("ただし、短期雇用者は△△" without "通勤手当")
- RAG retrieves general rule chunk, misses distant implicit exception

#### 2. Multi-hop Reasoning
Requires finding and combining information from multiple locations:
- "アルバイトの結婚祝金と出産祝金の合計はいくら？"
- "正社員とアルバイトの結婚祝金の差額は？"
- IBM notes: RAG "cannot discern connections between entities"

#### 3. Negation Understanding
Questions about what is NOT allowed:
- "アルバイトが申請できない福利厚生は何？"
- "パートタイムに適用されない制度を教えて"
- Requires understanding exclusions, not just finding matches

#### 4. Information NOT in Documents (Hallucination Test)
Should answer "該当情報なし" but often hallucinates:
- "アルバイトの退職金はいくら？" (not in any document)
- Enterprise RAG Challenge: "Systems must respond 'N/A' rather than fabricating information"

#### 5. Aggregation Queries
Summing or counting across chunks:
- "アルバイトが受けられる福利厚生の総額は？"
- IBM: "Vector databases lack built-in aggregation functions"

#### 6. Conditional Logic
Specific conditions buried in documents:
- "勤続6ヶ月のアルバイトは結婚祝金をもらえる？" (requires 1年以上)
- "週15時間勤務のパートは健康診断を受けられる？" (requires 20時間以上)

#### 7. Cross-document Queries
Information spans multiple files:
- "アルバイトが経費精算と慶弔見舞金の両方を申請する手続きは？"
- Requires retrieving from 経費精算規程.md AND 福利厚生規程.md

#### 8. Conflicting/Versioned Information
When multiple rules could apply:
- Old vs new policy versions
- General rule vs specific exception

### Key Research Insights

From LAC Watch (社内規程集RAG評価):
> 「1,000文字で分割した文書では Retrieval Correctness が最低の3.75となり、250文字程度に分割した方が良い結果となった」

From IBM:
> "Pure RAG is not really giving the optimal results that were expected... you don't know where you're chunking the data...you chunked in the middle of a table"

From 生成AI社内活用ナビ:
> 「企業が業務で利用している資料は、図表が多く含まれていたり、ページ数も多く複雑です」

---

## Next Plan: Dataset Restructuring

### Current Problem with Our Documents

Our "raw" documents in `regulations/` are actually well-structured:

```markdown
### 附則第3条（アルバイト・臨時スタッフに関する特則）  ← Explicit header!
アルバイトおよび臨時スタッフの通勤手当については、以下のとおり...  ← Re-states topic!
1. 申請方法は紙の届出書を使用...  ← Clearly numbered!
```

**Why RAG succeeds:**
- Query: "アルバイトの通勤手当の申請方法"
- Chunk contains: "アルバイト" + "通勤手当" + "申請" → Strong semantic match
- Result: Easy retrieval, correct answer

### What the Real Problem Looks Like

The challenge scenario describes realistic enterprise documents:

```markdown
第5条（申請方法）
従業員は、HR Portalから通勤届を提出する。届出は入社時および届出内容に
変更が生じた場合に行う...

第6条（支給額の計算）
通勤手当の支給額は、住居から勤務地までの最も経済的かつ合理的な経路に
基づき算定する。算定にあたっては、以下の基準を適用する...
[... 500 characters of calculation methods ...]

第7条（届出内容の変更）
届出内容に変更が生じた場合は、変更が生じた日から14日以内に届け出る
こととする。届出を怠った場合は...
[... 300 characters of change procedures ...]

第8条（届出の特例）
第5条の規定にかかわらず、短期雇用者については所定の届出書を
店舗責任者に提出する。届出書の様式は別紙のとおりとする。
```

**Why RAG fails:**
- Query: "アルバイトの通勤手当の申請方法"
- 第5条 chunk: "申請方法" + "届出" → Retrieved (wrong answer)
- 第8条 chunk: "短期雇用者" + "届出書" → NOT retrieved (no "通勤手当", no "申請")
- Result: Returns general rule, misses exception

### Strategy for Raw Dataset (`regulations/`)

#### 1. Bury exceptions in non-obvious sections
- Move from "附則（アルバイト特則）" to "第X条（届出の特例）" or "第X条（雑則）"
- Don't use employee type in section header

#### 2. Add long filler between general rule and exception
- Calculation methods, approval flows, change procedures
- Target: 500-1000 characters between related content
- Forces chunking to split general rule from exception

#### 3. Make exceptions implicit (no topic keyword repetition)
- ❌ Current: "アルバイトの通勤手当の申請は紙で行う"
- ✅ Realistic: "第5条の規定にかかわらず、短期雇用者については所定の届出書を使用する"
- Uses "前条", "第X条の規定にかかわらず" instead of restating topic

#### 4. Use indirect employee type references
- ❌ Current: "アルバイトおよび臨時スタッフ"
- ✅ Realistic: "短期雇用者", "非正規従業員", "第2条第3項に定める者"

### Strategy for Optimized Dataset (`regulations-optimized/`)

Keep well-structured with:
- Explicit headers: "## アルバイトの通勤手当"
- Q&A format where applicable
- Topic keywords repeated in each section
- Cross-references made explicit

### Expected Evaluation Results

| Dataset | Strategy | Expected Score | Reason |
|---------|----------|----------------|--------|
| Raw (restructured) | Standard | 30-50% | Can't find implicit exceptions |
| Raw (restructured) | Parent-child | 50-70% | Captures more context |
| Optimized | Standard | 80-90% | Explicit structure |
| Optimized | Parent-child | 85-95% | Best of both |

### New Test Query Categories

| Category | Example Query | Why It's Hard |
|----------|---------------|---------------|
| **Distant implicit** | アルバイトの通勤手当の申請方法は？ | Exception buried, no topic keyword |
| **Multi-hop** | 結婚祝金と出産祝金の合計は？ | Needs 2 retrievals + calculation |
| **Comparison** | 正社員とアルバイトの差額は？ | Needs 2 retrievals + comparison |
| **Negation** | 申請できない福利厚生は？ | Understanding exclusions |
| **Not in docs** | アルバイトの退職金は？ | Should say "規定なし" |
| **Conditional** | 勤続6ヶ月で結婚祝金もらえる？ | Specific condition check |
| **Cross-document** | 経費と慶弔の両方の申請手続きは？ | Multiple file retrieval |

### Files to Modify

1. `backend/app/data/regulations/*.md` - Restructure to realistic pattern
2. `backend/app/data/evaluation/test_queries_hard.json` - Add new query categories
3. Keep `backend/app/data/regulations-optimized/*.md` - Well-structured (the "solution")
