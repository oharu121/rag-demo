# Dev Notes - 2025-12-18

## Problem: Evaluation Tests Too Easy

### What We Observed
- Raw dataset + simple chunking strategy passed 90%+ of evaluation tests
- This undermines the demo's goal: proving that **data quality matters more than LLM optimization**
- Initial test set (`test_queries_light.json`) had only 4 queries - too few to differentiate

### Root Cause Analysis
1. **General queries are too easy** - Questions like "通勤手当の上限額は？" have answers in plain sight in the documents
2. **Exception cases are the real differentiator** - Questions about part-timers (アルバイト, パート) require finding 附則 (supplementary provisions) which are harder to retrieve
3. **Term matching was too strict** - Answers using different but correct phrasing (e.g., "認められていません" vs "できない") were marked as failures

### What We Changed

#### 1. Created Hard Test File (`test_queries_hard.json`)
- **10 exception-only queries** - all test edge cases for part-timers
- Topics covered: 通勤手当, 休暇, 経費, リモートワーク, 福利厚生, 服務
- Each query tests whether RAG can find:
  - Different rules for アルバイト vs 正社員
  - Specific limits (e.g., 2万円 vs 5万円)
  - Prohibited actions for certain employee types

#### 2. Added More Expected Term Variations
Before:
```json
"expected_answer_contains": ["認められない", "できない", "対象外"]
```

After:
```json
"expected_answer_contains": [
  "認められない", "できない", "対象外",
  "認められていません", "できません", "取得できません",
  "適用されません", "許可されていません"
]
```

#### 3. Improved Failure Feedback (`ScoringAnnotation.tsx`)
- Shows expected terms when test fails: `期待: 「term1」「term2」「term3」など`
- Shows prohibited terms if incorrectly found: `誤検出: term1, term2`
- Helps debug why tests fail

#### 4. Simplified Test Selection
- Removed test difficulty selector from UI
- Always runs hard test - simpler UX, clearer differentiation

### Files Modified
- `backend/app/config.py` - Changed to use `test_queries_hard.json`
- `backend/app/data/evaluation/test_queries_hard.json` - NEW: 10 exception queries
- `backend/app/data/evaluation/test_queries_medium.json` - Added term variations
- `frontend/app/components/ScoringAnnotation.tsx` - Shows expected terms on failure

### Expected Outcome
With harder exception-only queries:
- **Raw dataset**: Should score lower (附則 scattered, harder to retrieve)
- **Optimized dataset**: Should score higher (exceptions better structured)

This better demonstrates that data quality and structure matter more than chunking strategy alone.

---

## Streaming Evaluation Implementation

### Problem
- User had to wait 30-60 seconds for all results to appear at once
- Poor UX - no feedback during evaluation

### Solution
Implemented token-by-token streaming for evaluation (like regular chat):

#### Backend (`evaluation.py`)
- New SSE endpoint: `GET /api/evaluate/stream`
- Event types:
  - `query_start` - question about to be asked
  - `token` - answer token (streamed)
  - `query_done` - scoring for completed query
  - `complete` - final score summary

#### Frontend
- `api.ts`: Added `streamEvaluation()` async generator
- `useChat.ts`: Added streaming evaluation methods
- `ChatInterface.tsx`: Handles streaming events, shows progress

### Result
- Questions and answers appear one by one as they're processed
- Real-time token streaming for each answer
- Progress indicator shows "Testing 3/10..."
- Much better UX

---

## Evaluation Logs Not Showing in Hugging Face Spaces

### Problem
After deploying to HF Spaces, evaluation logs were not appearing in the container logs. Only HTTP request logs were visible:
```
INFO:     10.16.9.169:23145 - "GET /api/evaluate/stream?document_set=original&strategy=standard HTTP/1.1" 200 OK
```

No details about which questions were being asked, what answers were generated, or scoring results.

### Root Cause
Python's `logging.getLogger(__name__)` creates a logger but **doesn't attach handlers by default**. In containerized environments like Hugging Face Spaces, only `print()` statements with `flush=True` go to the visible container logs.

The code had:
```python
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Later in code...
logger.info(f"[Evaluation Stream] Q: {question}")  # NOT visible in HF logs!
```

The `/quick` endpoint had both `logger.info()` AND `print()` statements (so it worked), but the `/stream` endpoint only had `logger.info()` calls.

### Fix
Changed all `logger.info()` / `logger.error()` calls in the streaming endpoint to `print(..., flush=True)`:

```python
# Before (not visible in HF Spaces)
logger.info(f"[Evaluation Stream] Q: {question}")

# After (visible in HF Spaces)
print(f"[Eval {index+1}/{total}] Q: {question}", flush=True)
```

### Log Output Format
After the fix, logs now show:
```
[Eval] Starting evaluation: document_set=original, strategy=standard
[Eval] Loaded 10 test queries
[Eval 1/10] Q: アルバイトの通勤手当の申請方法を教えてください。
[Eval 1/10] A: アルバイトの通勤手当は紙の「通勤届（短期雇用者用）」を...
[Eval 1/10] ✓ found=['紙', '店舗責任者'] | missing=[] | prohibited=[]
[Eval 2/10] Q: アルバイトの通勤手当の上限額はいくらですか？
...
[Eval] Complete: 7/10 (70%)
```

### Key Takeaway
**In Docker/containerized environments, always use `print(..., flush=True)` for logs that need to be visible.** Python's logging module requires explicit handler configuration to output to stdout, which may not be set up by default in container environments.

### File Modified
- `backend/app/routers/evaluation.py` - Changed `logger.info()` to `print(..., flush=True)`
