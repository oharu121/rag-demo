# Dev Notes - 2025-12-20

## Key Learning: Runtime vs Build-time Parameters

Understanding which RAG parameters require rebuilding vector collections vs. which can be changed on-the-fly.

### Build-time Parameters (require rebuild)

These are baked into the vector database. Changing them requires rebuilding collections:

| Parameter | Why Rebuild Needed |
|-----------|-------------------|
| **Chunking strategy** | Chunks are stored as documents in the DB |
| **Chunk size / overlap** | Affects how text is split before embedding |
| **Embedding model** | Vector dimensions must match, embeddings are stored |
| **Document content** | The actual text being indexed |

### Runtime Parameters (no rebuild needed)

These are query-time or post-retrieval operations:

| Parameter | Why No Rebuild |
|-----------|---------------|
| **k (retrieval count)** | Just tells Chroma "return top k matches" |
| **Re-ranking** | Post-retrieval filtering, doesn't touch index |
| **Document set selection** | Just picks which collection to query |
| **Temperature, prompt** | LLM parameters, unrelated to index |

### How k Works

```python
# The vector DB stores ALL chunks
# k is just a query parameter
db.similarity_search_with_score(query, k=4)   # Return top 4
db.similarity_search_with_score(query, k=10)  # Return top 10
db.similarity_search_with_score(query, k=100) # Return top 100
```

Same index, different k values. No rebuild.

### How Re-ranking Works

```
Query → Vector Search (k=10) → Re-ranker → Top 4 → LLM
                ↑                    ↑
        Uses stored index    Runs at query time
        (build-time)         (runtime)
```

Re-ranking is a filter applied AFTER retrieval. The index doesn't know or care about it.

### Why This Matters

- **Fast experimentation**: Test different k values and reranking without waiting for rebuilds
- **A/B testing**: Same collection can serve different retrieval configurations
- **Cost optimization**: Only rebuild when chunking/embedding changes

---

## Implementing Re-ranking

Adding cross-encoder re-ranking as an optional toggle that works with any chunking strategy.

### Architecture Decision

Re-ranking as a **toggle**, not a chunking strategy because:
- Works ON TOP OF any chunking strategy
- User can combine: `large chunks + reranking` or `standard + reranking`
- More flexible for experimentation

### Technical Details

- **Model**: `cross-encoder/ms-marco-MiniLM-L-6-v2` via sentence-transformers
- **Already installed**: `sentence-transformers>=3.0` in dependencies
- **Flow**: Retrieve k=10, re-rank, return top 4
- **Default k=4**: When reranking disabled, use normal k=4

### UI Design

Three-section layout:
```
[データセット選択]     ← Which documents (original/optimized)
[チャンキング戦略]     ← How to chunk (4 cards)
[リランキング]         ← Post-retrieval filter (2 cards: on/off)
```

---

## Implementation Details

### Files Modified

| File | Change |
|------|--------|
| `backend/app/services/reranker_service.py` | **NEW** - Cross-encoder reranking service |
| `backend/app/config.py` | Added reranker settings |
| `backend/app/models/schemas.py` | Added `use_reranking` to ChatRequest |
| `backend/app/services/rag_service.py` | Retrieve k=10 when reranking, apply reranker |
| `backend/app/routers/chat.py` | Pass `use_reranking` to stream_query |
| `backend/app/routers/evaluation.py` | Added `use_reranking` param to both endpoints |
| `frontend/lib/api.ts` | Added `useReranking` to streamChat and streamEvaluation |
| `frontend/hooks/useChat.ts` | Added `useReranking` to ChatOptions |
| `frontend/app/components/ChatInterface.tsx` | Added state + passing to components |
| `frontend/app/components/DatasetSelector.tsx` | Added purple-themed reranking radio cards |

### Backend: RerankerService

```python
# backend/app/services/reranker_service.py

class RerankerService:
    def __init__(self):
        self._model = None  # Lazy load

    def _get_model(self):
        if self._model is None:
            from sentence_transformers import CrossEncoder
            self._model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
        return self._model

    def rerank(self, query: str, documents: list[tuple[Document, float]], top_k: int = 4):
        # Score each doc against query using cross-encoder
        model = self._get_model()
        pairs = [(query, doc.page_content) for doc, _ in documents]
        scores = model.predict(pairs)

        # Sort by cross-encoder score (higher is better)
        scored_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)

        # Return top_k with new scores
        return [(doc, float(score)) for (doc, _), score in scored_docs[:top_k]]
```

Key design decisions:
- **Lazy loading**: Model loaded on first use (~80MB, ~2s load time)
- **Handles metadata**: Uses `original_content` or `parent_content` when available
- **Returns new scores**: Cross-encoder scores replace similarity scores

### Backend: Config Settings

```python
# backend/app/config.py

# Reranking Config
reranker_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
reranker_initial_k: int = 10  # Retrieve more candidates for reranking
reranker_top_k: int = 4       # Final count after reranking
```

### Backend: RAG Service Integration

```python
# backend/app/services/rag_service.py (stream_query and query methods)

# Determine k based on reranking
retrieval_k = self.settings.reranker_initial_k if use_reranking else None

# Retrieve documents
docs_with_scores = vectorstore_service.similarity_search_with_score(
    question, document_set=document_set, strategy=strategy, k=retrieval_k
)

# Apply reranking if enabled
if use_reranking and docs_with_scores:
    from app.services.reranker_service import get_reranker_service
    reranker = get_reranker_service()
    docs_with_scores = reranker.rerank(
        question, docs_with_scores, top_k=self.settings.reranker_top_k
    )
```

### Frontend: API Updates

```typescript
// frontend/lib/api.ts

export async function* streamChat(
  message: string,
  history: Pick<Message, "role" | "content">[],
  options?: {
    documentSet?: DocumentSet;
    strategy?: ChunkingStrategy;
    useReranking?: boolean;  // NEW
  }
) {
  // ...
  body: JSON.stringify({
    // ...
    use_reranking: options?.useReranking,  // snake_case for backend
  }),
}

export async function* streamEvaluation(
  documentSet: DocumentSet,
  strategy: ChunkingStrategy,
  useReranking: boolean = false  // NEW
) {
  const response = await fetch(
    `${baseUrl}/api/evaluate/stream?document_set=${documentSet}&strategy=${strategy}&use_reranking=${useReranking}`
  );
}
```

### Frontend: DatasetSelector UI

```tsx
// frontend/app/components/DatasetSelector.tsx

const RERANKING_INFO = {
  disabled: {
    name: "リランキングなし",
    description: "ベクトル検索の結果をそのまま使用",
    pros: "高速",
    cons: "精度は検索依存",
  },
  enabled: {
    name: "リランキングあり",
    description: "Cross-Encoderで検索結果を再評価",
    pros: "精度向上",
    cons: "少し遅い",
  },
} as const;

// Purple theme for reranking cards (blue for chunking, purple for reranking)
className={`... ${isSelected ? "border-purple-400 bg-purple-50/80" : "..."}`}
```

### Data Flow

```
User clicks "リランキングあり"
    ↓
ChatInterface: setUseReranking(true)
    ↓
DatasetSelector: purple card selected
    ↓
User sends message
    ↓
handleSendMessage({ documentSet, strategy, useReranking: true })
    ↓
streamChat() → POST /chat with use_reranking=true
    ↓
rag_service.stream_query(use_reranking=true)
    ↓
similarity_search(k=10) → reranker.rerank() → top 4 → LLM
    ↓
Response streamed back to user
```

### Color Scheme

| Section | Color Theme |
|---------|-------------|
| Dataset (original) | Gray |
| Dataset (optimized) | Emerald/Green |
| Chunking Strategy | Blue |
| Reranking | Purple |

This visual distinction helps users understand the three-layer architecture:
1. **What** to search (dataset)
2. **How** to chunk (strategy)
3. **How** to filter (reranking)

---

## Re-ranking Evaluation Results

### Test Configuration

- **Dataset**: Original (6 documents)
- **Strategy**: Standard chunking
- **Reranking**: Enabled (Cross-Encoder)
- **Questions**: 15 evaluation questions

### Results

| Configuration | Score | Percentage |
|--------------|-------|------------|
| Standard alone | 11/15 | 73.3% |
| Standard + Reranking | 10/15 | 66.7% |

**Net effect: -1 correct answer (worse)**

### Detailed Analysis

Re-ranking caused **3 regressions** and only **1 improvement**:

| Question | Without Reranking | With Reranking | Change |
|----------|------------------|----------------|--------|
| Q4: 正社員の通勤手当の条件 | ✓ | ✗ | Regression |
| Q9: リモートワーク時の通勤手当 | ✗ | ✓ | Improvement |
| Q11: パートタイムの有給休暇 | ✓ | ✗ | Regression |
| Q13: パートの年末年始休暇 | ✓ | ✗ | Regression |

### Why Re-ranking Didn't Help

1. **Alias mismatch problem remains**: Re-ranking reorders documents that were **already retrieved**. If the right document wasn't retrieved in the first place (k=10), reranking can't find it.

2. **Cross-encoder limitations**: The model evaluates query-document relevance, but can't:
   - Resolve terminology aliases (「パート」→「パートタイム」)
   - Infer multi-hop relationships (正社員→就業規則→附則)

3. **False confidence**: Re-ranking sometimes promotes plausible-sounding but incorrect chunks over the actually correct ones.

### Key Insight

> Re-ranking is a **precision** tool, not a **recall** tool.
>
> It helps when you retrieve good candidates but rank them poorly.
> It doesn't help when you fail to retrieve the right candidates at all.

Our problem is **retrieval failure** (alias mismatch), not **ranking failure**. That's why:
- Hypothetical Questions (80%) helps - generates aliases at index time
- Optimized data (93%) helps - restructures data for better retrieval
- Re-ranking (66.7%) doesn't help - can't fix what wasn't retrieved

### Updated Strategy Comparison

| Strategy | Original Data | Optimized Data | Notes |
|----------|--------------|----------------|-------|
| Standard | 73.3% (11/15) | 93.3% (14/15) | Baseline |
| Standard + Reranking | 66.7% (10/15) | - | Worse than baseline |
| Large Chunks | 73.3% (11/15) | - | Same as standard |
| Parent-Child | 73.3% (11/15) | - | Same as standard |
| Parent-Child + Reranking | 86.7% (13/15) | - | **Surprising improvement!** |
| Hypothetical Questions | 80.0% (12/15) | - | Best chunking-only strategy |

### Surprising Finding: Parent-Child + Reranking

Parent-Child alone: 73.3% (11/15)
Parent-Child + Reranking: **86.7% (13/15)** → +4 correct answers!

This is unexpected. Why might this work?

**Hypothesis**: Parent-Child retrieves more context (parent chunks) but the initial ranking may not prioritize the best parents. Re-ranking with Cross-Encoder can:
1. Better evaluate relevance of parent chunks that contain the answer
2. Demote parent chunks that are tangentially related but not useful

The combination leverages both:
- **Parent-Child**: Cast a wider net (more context in chunks)
- **Reranking**: Precision filtering to surface the best results

### Conclusion

Re-ranking results vary by chunking strategy:

| Combination | Effect |
|-------------|--------|
| Standard + Reranking | **Worse** (-1) |
| Parent-Child + Reranking | **Better** (+4) |

This suggests:
- Re-ranking is NOT universally helpful or harmful
- The chunking strategy significantly affects whether re-ranking helps
- Parent-Child's larger context chunks may benefit more from semantic re-ranking
- Standard's smaller chunks may already be well-ranked by vector similarity

**Recommendation**: Test re-ranking with each chunking strategy before deciding to enable/disable it.

---

## Understanding Result Variance in RAG Evaluation

### Observation

Running the same evaluation multiple times can produce slightly different results (e.g., 11/15 vs 10/15 for the same configuration). This section explains why.

### Sources of Non-Determinism

#### 1. LLM Temperature (Primary Cause)

The LLM generates answers with `temperature=0.3`, which introduces controlled randomness.

```python
# In config.py
llm_temperature: float = 0.3
```

**How Temperature Works:**

Temperature controls the probability distribution over the next token:

```
Temperature = 0.0: Always pick the highest probability token (deterministic)
Temperature = 0.3: Slightly randomize, but still favor high-probability tokens
Temperature = 1.0: Sample according to true probabilities (more creative/random)
Temperature > 1.0: Flatten probabilities (very random, often incoherent)
```

**Mathematical Effect:**

```
Original logits:     [2.0, 1.5, 0.5, 0.1]
After softmax:       [0.47, 0.29, 0.10, 0.07]  ← probabilities

With temperature=0.3:
Adjusted logits:     [6.67, 5.0, 1.67, 0.33]   ← divide by 0.3
After softmax:       [0.82, 0.15, 0.02, 0.01]  ← much more peaked

With temperature=1.0:
Adjusted logits:     [2.0, 1.5, 0.5, 0.1]      ← unchanged
After softmax:       [0.47, 0.29, 0.10, 0.07]  ← original distribution
```

Lower temperature = more deterministic, but still not 100% reproducible.

**Impact on Evaluation:**

```
# Same retrieved chunks, different LLM outputs due to temperature:
Run 1: "上限額は月額20,000円です"      → ✓ matches "20,000円"
Run 2: "上限額は2万円となっています"    → ✓ matches "2万円"
Run 3: "上限は二万円です"               → ✗ neither keyword matched
Run 4: "月に2万円が上限となります"      → ✓ matches "2万円"
```

The answer is semantically correct in all cases, but keyword matching may fail.

#### 2. Retrieval Pipeline (Fully Deterministic)

**Important clarification**: The entire retrieval pipeline is deterministic.

| Component | Deterministic? | Notes |
|-----------|----------------|-------|
| Query embedding | ✅ Yes | SentenceTransformer: same input = same vector |
| Chroma search | ✅ Yes | Exact cosine similarity (not approximate) |
| Score calculation | ✅ Yes | Pure math on vectors |
| Ranking | ✅ Yes | Sorted by score |
| Cross-Encoder reranking | ✅ Yes | Same inputs = same scores |

```python
# This ALWAYS returns the SAME chunks in the SAME order for a given query
vectorstore.similarity_search_with_score(query, k=4)
```

**Note on HNSW**: Chroma can use HNSW (approximate nearest neighbor) for very large collections (millions of vectors), but for demo-sized collections like ours (~100-200 chunks), it uses **exact search**.

#### 3. Where Variance Actually Comes From

The ONLY source of variance is the LLM answer generation:

```
Query
  ↓
Embedding (deterministic)
  ↓
Vector Search (deterministic) → Same chunks every time
  ↓
Reranking (deterministic) → Same order every time
  ↓
LLM with temperature=0.3 → VARIANCE INTRODUCED HERE
  ↓
Answer text varies slightly
  ↓
Keyword matching may pass or fail depending on wording
```

### Quantifying the Variance

Based on observations:

| Configuration | Typical Range | Variance |
|---------------|---------------|----------|
| Standard | 10-12/15 | ±1 question |
| Parent-Child + Reranking | 12-14/15 | ±1 question |
| Optimized + Standard | 13-15/15 | ±1 question |

**Relative rankings remain stable** even when absolute scores vary.

### How to Reduce Variance

#### Option 1: Set Temperature to 0

```python
# In config.py
llm_temperature: float = 0.0  # Fully deterministic
```

**Trade-offs of Temperature = 0:**

| Aspect | Temperature = 0 | Temperature = 0.3 |
|--------|-----------------|-------------------|
| Reproducibility | ✅ Fully deterministic | ❌ Slight variance |
| Natural language | ❌ Can sound robotic/repetitive | ✅ More natural phrasing |
| Handling ambiguity | ❌ Always picks same interpretation | ✅ Can explore alternatives |
| Error patterns | ❌ Same mistakes every time | ✅ May recover on retry |
| User experience | ❌ Feels mechanical | ✅ Feels more conversational |

**Recommendation for RAG:**
- **Demo/testing**: Use `temperature=0` for reproducible results
- **Production**: Use `temperature=0.1-0.3` for better UX
- **Our current setting**: `temperature=0.3` - good balance for demo naturalness

#### Option 2: Run Multiple Evaluations

```bash
# Run 3 times and average
Run 1: 11/15
Run 2: 12/15
Run 3: 11/15
Average: 11.3/15 (75.6%)
```

#### Option 3: Use More Lenient Keyword Matching

```python
# Current: exact keyword match
required_keywords = ["20,000円", "2万円"]

# Better: include variations
required_keywords = ["20,000円", "2万円", "二万円", "20000円"]
```

#### Option 4: Semantic Similarity Scoring

Instead of keyword matching, use embedding similarity:

```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer("all-MiniLM-L6-v2")

expected = "通勤手当の上限は2万円"
actual = "上限額は月額二万円です"

similarity = cosine_similarity(
    model.encode(expected),
    model.encode(actual)
)
# similarity ≈ 0.85 → considered correct
```

### Implications for the Demo

1. **Don't over-promise exact numbers**
   - Say "approximately 73%" not "exactly 73.3%"
   - Focus on relative improvements, not absolute scores

2. **Run evaluation before demo**
   - Verify current scores match expectations
   - Have backup talking points if numbers differ

3. **Explain variance if asked**
   - "LLM answers vary slightly due to temperature setting"
   - "Relative rankings are stable across runs"

4. **For production systems**
   - Use temperature=0 for consistency
   - Implement semantic similarity scoring
   - A/B test with real users for true accuracy metrics

### Key Takeaway

> **Variance is expected and realistic.**
>
> Production RAG systems also show variance. The goal is to demonstrate
> that data quality improvements consistently outperform baseline,
> not to achieve perfectly reproducible numbers.

---

## Why Data Optimization Beats All Other Strategies

### The Core Insight

All chunking strategies (Standard, Large, Parent-Child) require the LLM to make an **inference step**:

> "Does '第2条の2に定める者' refer to アルバイト?"

This inference is **non-deterministic** and can fail even with the same retrieved chunks.

### The Problem: LLM Reasoning Variance

```
Retrieved Chunks (SAME every time for Parent-Child):
  - Chunk 1: "第2条の2に定める者については、上限額を月額20,000円とする"
  - Chunk 2: "通勤手当の上限額は月額50,000円とする"
  - Chunk 3: "本規程において「従業員」とは、正社員、契約社員..."

Query: "アルバイトの通勤手当の上限は？"

LLM Run 1:
  Reasoning: "第2条の2に定める者 = 短期雇用者 = アルバイト... therefore 20,000円"
  Answer: "アルバイトの通勤手当の上限は20,000円です。" ✓

LLM Run 2:
  Reasoning: "I see 50,000円 and 20,000円, but which applies to アルバイト is unclear..."
  Answer: "この情報はドキュメントに含まれていません。" ✗

LLM Run 3:
  Reasoning: "The document mentions 従業員 and 50,000円..."
  Answer: "通勤手当の上限は50,000円です。" ✗ (wrong - that's for 正社員)
```

The LLM sometimes makes the connection, sometimes doesn't. This is **inherent uncertainty** when the source data requires interpretation.

### Why Optimized Data Eliminates This Uncertainty

With preprocessed documents split by employee type:

```
Retrieved Chunk (from 通勤手当規程_アルバイト.md):
  "アルバイトの通勤手当の上限は20,000円です。"

Query: "アルバイトの通勤手当の上限は？"

LLM (no inference needed):
  Answer: "アルバイトの通勤手当の上限は20,000円です。" ✓ (always)
```

**No ambiguity. No inference. No variance.**

### Comparison: Inference Required vs Not Required

| Approach | LLM Task | Inference Required | Failure Mode |
|----------|----------|-------------------|--------------|
| Original + Any Chunking | "Connect アルバイト to 第2条の2に定める者" | High | May fail to connect |
| Hypothetical Questions | "Match query to generated questions" | Medium | Questions may not cover all aliases |
| Reranking | "Pick best chunk" | Medium | May promote wrong chunk |
| **Optimized Data** | "Read the answer directly" | **None** | **Almost no failure mode** |

### The Mathematical Argument

Let's model the probability of correct answer:

```
P(correct) = P(retrieve right chunk) × P(LLM interprets correctly)

For Original Data:
  P(retrieve) ≈ 0.85 (good enough with reranking)
  P(interpret) ≈ 0.85 (LLM must infer alias)
  P(correct) = 0.85 × 0.85 = 0.72 (72%)

For Optimized Data:
  P(retrieve) ≈ 0.98 (document name matches query)
  P(interpret) ≈ 0.98 (answer is explicit, no inference)
  P(correct) = 0.98 × 0.98 = 0.96 (96%)
```

This explains the observed gap: ~73% (original) vs ~93% (optimized).

### Why This Matters for the Demo

The variance you observed (Parent-Child sometimes works, sometimes says "information not found") is **proof that the problem is data quality, not LLM quality**.

**Talking point:**
> "Notice how the same chunking strategy sometimes works, sometimes doesn't?
> That's because the LLM has to infer that '第2条の2に定める者' means 'アルバイト'.
> This inference can fail randomly.
>
> With optimized data, no inference is needed - the document literally says 'アルバイト'.
> That's why data optimization achieves 93% while chunking strategies plateau at ~73-86%."

### Implications for Production Systems

1. **Chunking optimization has a ceiling**
   - No matter how good your chunking, if the data requires interpretation, there's inherent uncertainty
   - Best observed: Parent-Child + Reranking = 86.7%

2. **Data preprocessing removes the ceiling**
   - When the answer is explicit, LLM variance becomes negligible
   - Observed: 93.3% and very stable

3. **Cost-benefit is clear**
   - Chunking strategies: Ongoing runtime complexity, moderate improvement
   - Data preprocessing: One-time effort, maximum improvement

### Summary

| Strategy | Requires LLM Inference | Stability | Max Accuracy |
|----------|----------------------|-----------|--------------|
| Standard | High | Low | ~73% |
| Parent-Child | High | Low | ~73% |
| Parent-Child + Reranking | Medium | Medium | ~87% |
| Hypothetical Questions | Medium | Medium | ~80% |
| **Optimized Data** | **None** | **High** | **~93%+** |

> **Key Insight**: The best RAG optimization is making the answer obvious to the LLM, not making the LLM smarter at finding hidden answers.
